<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Bayes | Dexter Cao 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Dexter Cao">
    
    

    <meta name="description" content="Bayesian linear regressionExample 1. Simple univariate regression - real-estate price predictionConsider an example of predicting prices of individual homes $y$ vs their sizes $x$:$$y=w x+\varepsilon,">
<meta property="og:type" content="article">
<meta property="og:title" content="Bayes | Dexter Cao">
<meta property="og:url" content="http://yunongcao.github.io/2016/04/04/Bayes/index.html">
<meta property="og:site_name" content="Dexter Cao">
<meta property="og:description" content="Bayesian linear regressionExample 1. Simple univariate regression - real-estate price predictionConsider an example of predicting prices of individual homes $y$ vs their sizes $x$:$$y=w x+\varepsilon,">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_6_1.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_32_0.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_43_0.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_48_0.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_51_1.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_54_0.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_67_0.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_69_0.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_73_0.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_74_0.png">
<meta property="og:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_77_0.png">
<meta property="og:updated_time" content="2016-04-04T22:49:15.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bayes | Dexter Cao">
<meta name="twitter:description" content="Bayesian linear regressionExample 1. Simple univariate regression - real-estate price predictionConsider an example of predicting prices of individual homes $y$ vs their sizes $x$:$$y=w x+\varepsilon,">
<meta name="twitter:image" content="http://yunongcao.github.io/NBsession2_files/NBsession2_6_1.png">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css">

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Dexter Cao</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          A Blog Project
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">Blog</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">About</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">Archive</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/yunongcao" title="Huno on GitHub">
          <i class='icon icon-social-github'></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

    <!-- China social icon -->
    <!--
    
      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>

      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    -->



  </ul>
</nav>



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Bayes</h1>

    

    <div class="post-meta">
      <time datetime="2016-04-04" class="post-meta__date date">2016-04-04</time> 

      <span class="post-meta__tags tags">

          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Machine-Learning/">Machine Learning</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h1 id="Bayesian-linear-regression"><a href="#Bayesian-linear-regression" class="headerlink" title="Bayesian linear regression"></a>Bayesian linear regression</h1><h2 id="Example-1-Simple-univariate-regression-real-estate-price-prediction"><a href="#Example-1-Simple-univariate-regression-real-estate-price-prediction" class="headerlink" title="Example 1. Simple univariate regression - real-estate price prediction"></a>Example 1. Simple univariate regression - real-estate price prediction</h2><p>Consider an example of predicting prices of individual homes $y$ vs their sizes $x$:<br>$$<br>y=w x+\varepsilon,<br>$$<br>given a series of single-dimensional observations $D={(y_i,x_i), i=1..N}$, where $w$ is the slope coefficient to fit (in our case - average price of the square foot) and $\epsilon$ is the error term (deviation of the specific price from the predicted value), assuming that<br>$$<br>\varepsilon\sim {\cal N}(0,\sigma^2).<br>$$<br>As we remember ordinary least square (OLS) estimate in the univariate case gives a simple estimate $\hat{w}$ for the slope coefficient $w$:<br>$$<br>\hat{w}=\frac{\sum_i y_i x_i}{\sum_i x_i^2}.<br>$$<br>After we get $w=\hat{w}$ it is easy to estimate the standard error<br>$$<br>\sigma=\sqrt{\frac{\sum_i (y_i-\hat{w} x_i)^2}{N}}.<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%pylab inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> pandas.io.data <span class="keyword">as</span> web</span><br><span class="line"><span class="keyword">import</span> Quandl</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">import</span> statsmodels.formula.api <span class="keyword">as</span> smf</span><br><span class="line"><span class="keyword">import</span> pymc3 <span class="keyword">as</span> pm</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rnd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">from</span> pandas.stats.api <span class="keyword">import</span> ols</span><br><span class="line"><span class="keyword">import</span> pylab</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>]=(<span class="number">10.0</span>,<span class="number">8.0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Populating the interactive namespace from numpy and matplotlib


/Users/Dexter/anaconda/lib/python2.7/site-packages/pandas/io/data.py:33: FutureWarning: 
The pandas.io.data module is moved to a separate package (pandas-datareader) and will be removed from pandas in a future version.
After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.
  FutureWarning)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#read real estate sample data</span></span><br><span class="line">REsample1 = pd.read_csv(<span class="string">'data/REsample1.csv'</span>)</span><br><span class="line">REsample1.head()</span><br></pre></td></tr></table></figure>
<div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>Unnamed: 0</th><br>      <th>borough</th><br>      <th>neighborhood</th><br>      <th>building_class_category</th><br>      <th>tax_class_present</th><br>      <th>block</th><br>      <th>lot</th><br>      <th>easement</th><br>      <th>building_class_present</th><br>      <th>address</th><br>      <th>…</th><br>      <th>land_sq_feet</th><br>      <th>gross_sq_feet</th><br>      <th>year_built</th><br>      <th>tax_class_time_sale</th><br>      <th>build_class_time_sale</th><br>      <th>sale_price</th><br>      <th>sale_date</th><br>      <th>serialid</th><br>      <th>sale_year</th><br>      <th>sale_month</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>5631</td><br>      <td>3</td><br>      <td>BAY RIDGE</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>5871</td><br>      <td>91</td><br>      <td>NaN</td><br>      <td>A5</td><br>      <td>24 BAY RIDGE PLACE</td><br>      <td>…</td><br>      <td>1600</td><br>      <td>1400</td><br>      <td>1910</td><br>      <td>1</td><br>      <td>A5</td><br>      <td>585000</td><br>      <td>2009-10-29</td><br>      <td>5631</td><br>      <td>9</td><br>      <td>9</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>5635</td><br>      <td>3</td><br>      <td>BAY RIDGE</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>5882</td><br>      <td>38</td><br>      <td>NaN</td><br>      <td>A5</td><br>      <td>7014 LOUISE TERRACE</td><br>      <td>…</td><br>      <td>1169</td><br>      <td>1224</td><br>      <td>1925</td><br>      <td>1</td><br>      <td>A5</td><br>      <td>515000</td><br>      <td>2009-10-15</td><br>      <td>5635</td><br>      <td>9</td><br>      <td>9</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>5636</td><br>      <td>3</td><br>      <td>BAY RIDGE</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>5882</td><br>      <td>50</td><br>      <td>NaN</td><br>      <td>A5</td><br>      <td>7009 LOUISE TERRACE</td><br>      <td>…</td><br>      <td>945</td><br>      <td>1836</td><br>      <td>1925</td><br>      <td>1</td><br>      <td>A5</td><br>      <td>499900</td><br>      <td>2009-04-20</td><br>      <td>5636</td><br>      <td>9</td><br>      <td>3</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>5637</td><br>      <td>3</td><br>      <td>BAY RIDGE</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>5882</td><br>      <td>80</td><br>      <td>NaN</td><br>      <td>A5</td><br>      <td>7002 COLONIAL ROAD</td><br>      <td>…</td><br>      <td>1121</td><br>      <td>1330</td><br>      <td>1925</td><br>      <td>1</td><br>      <td>A5</td><br>      <td>525000</td><br>      <td>2009-09-02</td><br>      <td>5637</td><br>      <td>9</td><br>      <td>8</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>5638</td><br>      <td>3</td><br>      <td>BAY RIDGE</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>5882</td><br>      <td>81</td><br>      <td>NaN</td><br>      <td>A5</td><br>      <td>7004 COLONIAL ROAD</td><br>      <td>…</td><br>      <td>1118</td><br>      <td>1224</td><br>      <td>1925</td><br>      <td>1</td><br>      <td>A5</td><br>      <td>400000</td><br>      <td>2009-07-24</td><br>      <td>5638</td><br>      <td>9</td><br>      <td>6</td><br>    </tr><br>  </tbody><br></table><br><p>5 rows × 25 columns</p><br></div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#first fit the regression for the entire sample</span></span><br><span class="line">RE1w=sum(REsample1.sale_price*REsample1.gross_sq_feet)/sum(REsample1.gross_sq_feet**<span class="number">2</span>)</span><br><span class="line"><span class="comment">#estimate the standard error sigma</span></span><br><span class="line">sigma=std(REsample1.sale_price-REsample1.gross_sq_feet*RE1w)</span><br><span class="line">print(<span class="string">'Estimated w=&#123;0&#125;, sigma=&#123;1&#125;'</span>.format(RE1w,sigma))</span><br></pre></td></tr></table></figure>
<pre><code>Estimated w=423.262774919, sigma=387363.577755
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#now redo the regression for just the first 100 records</span></span><br><span class="line">REsample11=REsample1.loc[<span class="number">0</span>:<span class="number">99</span>]</span><br><span class="line">RE11w=sum(REsample11.sale_price*REsample11.gross_sq_feet)/sum(REsample11.gross_sq_feet**<span class="number">2</span>)</span><br><span class="line">print(<span class="string">'Estimated w=&#123;0&#125;'</span>.format(RE11w))</span><br></pre></td></tr></table></figure>
<pre><code>Estimated w=369.713904612
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#visualize the fit</span></span><br><span class="line">REsample11.plot(kind=<span class="string">'scatter'</span>,x=<span class="string">'gross_sq_feet'</span>,y=<span class="string">'sale_price'</span>)</span><br><span class="line">y1=RE11w*REsample11.gross_sq_feet</span><br><span class="line">plt.plot(REsample11.gross_sq_feet,y1,<span class="string">'r-'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x10a81fe90&gt;]
</code></pre><p><img src="NBsession2_files/NBsession2_6_1.png" alt="png"></p>
<p>We see quite a deviation in the estimates $\hat{w}$ obtained - so the OLS estimate depends on the sample choice and uses to vary from one sample to another. So for any particular choice of the training sample there is no guarantee that the estimate $\hat{w}$ is indeed a true value for $w$ - we should admit uncertainty in our estimate, which OLS does not really allow to make, but the further Bayesian framework will.</p>
<h1 id="Bayesian-univariate-regression"><a href="#Bayesian-univariate-regression" class="headerlink" title="Bayesian univariate regression"></a>Bayesian univariate regression</h1><p>Also imagine that we already have certain prior knowledge about $w$ for example this would be the case if we already studied some sample of house prices before. Can we take this into account somehow? The prior knowledge can be expressed through a prior distribution $p(w)$.</p>
<p>Consider a probabilistic version of a univariate regression:<br>$$<br>y\sim {\cal N}(w x,\sigma^2),<br>$$<br>assuming that the variance $\sigma^2$ is given or at least independent on $w$ and the only real-valued single dimensional coefficient to estimate based on a series of single-dimensional observations $D=(Y,X)={(y_i,x_i), i=1..N}$ is $w$. In a similar manner as we just did for fitting a normal distribution, each single observation changes a given conjugate normal prior<br>$$<br>w\sim {\cal N}(w^<em>,\sigma^</em>)<br>$$<br>to a posterior<br>$$<br>p(w|y=y_i, x=x_i)\sim p(y=y_i|w,x=x_i)p(w)\sim e^{-\frac{(y_i-w x_i)^2}{2\sigma^2}-\frac{(w-w^<em>)^2}{2(\sigma^</em>)^2}}\sim<br>$$$$<br>\sim e^{-w^2 \frac{x_i^2(\sigma^<em>)^2+\sigma^2}{2\sigma^2(\sigma^</em>)^2}+w \frac{(\sigma^<em>)^2 y_i x_i+\sigma^2 w^</em>}{2\sigma^2(\sigma^<em>)^2}}<br>\sim e^{\frac{\left(w-\frac{ \sigma^{-2} y_i x_i +(\sigma^</em>)^{-2} w^<em>}{ \left(x_i^2\sigma^{-2}+(\sigma^</em>)^{-2}\right)}\right)^2}{\frac{2 }{(\sigma^*)^{-2}+x_i^2\sigma^{-2}}}}<br>$$<br>so posterior distribution is also normal.</p>
<p>In order to add all the observations from $D$ one can either inerate the above process or consider a single equation based on a multi-variable probability distribution, which is easy to handle assuming that different observations are independent:<br>$$<br>p(w|Y, X)\sim p(Y|w,X)p(w)=p(w)\prod\limits_i p(y=y_i|w,x=x_i)\sim e^{-\sum\limits_i \frac{(y_i-w x_i)^2}{2\sigma^2}-\frac{(w-w^<em>)^2}{2(\sigma^</em>)^2}}=e^{-\frac{RSS(w)}{2\sigma^2}-\frac{(w-w^<em>)^2}{2(\sigma^</em>)^2}}\hspace{5ex} (1)<br>$$<br>If one needs a specific estimate $\hat{w}$ for $w$ then it could be found as the most probable value from<br>$$<br>\hat{w}=argmax_w p(w|Y, X)=argmin_w\left[\frac{RSS(w)}{2\sigma^2}+\frac{(w-w^<em>)^2}{2(\sigma^</em>)^2}\right]\hspace{5ex} (2)<br>$$</p>
<p>$$<br>p(w|Y, X)\sim e^{-w^2 \frac{\sum_i x_i^2(\sigma^<em>)^2+\sigma^2}{2\sigma^2(\sigma^</em>)^2}+w \frac{(\sigma^<em>)^2 \sum_i y_i x_i+\sigma^2 w^</em>}{2\sigma^2(\sigma^<em>)^2}}\sim e^{\frac{\left(w-\frac{ \sigma^{-2} \sum_i y_i x_i +(\sigma^</em>)^{-2} w^<em>}{\left(\sum_i x_i^2\sigma^{-2}+(\sigma^</em>)^{-2}\right)}\right)^2}{\frac{2}{(\sigma^*)^{-2}+\sum_i x_i^2\sigma^{-2}}}}\sim<br>$$</p>
<p>$$<br>\sim {\cal N}\left(\frac{ \sigma^{-2} \sum_i y_i x_i +(\sigma^<em>)^{-2} w^</em>}{\left(\sum_i x_i^2\sigma^{-2}+(\sigma^<em>)^{-2}\right)},<br>\frac{1}{\sqrt{(\sigma^</em>)^{-2}+\sum_i x_i^2\sigma^{-2}}}\right)<br>$$</p>
<h3 id="Uninformed-prior"><a href="#Uninformed-prior" class="headerlink" title="Uninformed prior"></a>Uninformed prior</h3><p>In case of an unfinformed prior with $\sigma^*\to \infty$ the above estimate (2) is tends to be equivalent to the one given by the regular OLS, i.e. to<br>$$<br>RSS(w)\to \min<br>$$<br>However in addition to the single best fit slope coefficient estimate (2), Bayesian regression also provides an entire probabilistic normal distribution<br>$$<br>w\sim {\cal N}\left(\frac{\sum_i x_i y_i}{\sum_i x_i^2},\frac{\sigma}{\sqrt{\sum_i x_i^2}}\right)<br>$$<br>for the possible values of $w$, being able to explain from the rigorous probabilistic standpoint the meaning behind the confidence intervals and hypothesis testing.</p>
<p>One point here is that so far we consider $\sigma$ as given. There is a more sophisticated Bayesian framework which allows to estimate it together with $w$ (conceptually just enough to consider a vector of two latent variables $w,\sigma$ instead of $w$ and specify its joint prior distribution, although maths is much heavier in that case). For now we can simply rely on the streightforward max-likelihood estimate<br>$$<br>\hat{\sigma}^2=\frac{RSS(\hat{w})}{N}<br>$$<br>also being an actual sample standard deviation of the set of observed prediction errors for the optimal $w=\hat{w}$.</p>
<h2 id="Example-1-revisited-Distribution-for-the-sq-foot-price"><a href="#Example-1-revisited-Distribution-for-the-sq-foot-price" class="headerlink" title="Example 1 revisited. Distribution for the sq.foot price"></a>Example 1 revisited. Distribution for the sq.foot price</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#we just need to select sigma here - take the one we estimated for the entire sample</span></span><br><span class="line">RE11s=sigma/(sum(REsample11.gross_sq_feet**<span class="number">2</span>)**<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'w ~ N(&#123;0&#125;,&#123;1&#125;)'</span>.format(RE11w,RE11s))</span><br></pre></td></tr></table></figure>
<pre><code>w ~ N(369.713904612,18.293523802)
</code></pre><p>notice that the standard deviation we found for $w$ depends on the choice of $\sigma$ and might become different if we take $\sigma$ based on the considered subsample rather than the entire sample as we did.</p>
<h2 id="Example-2-Bayesian-regression-example-tuning-the-houses-price-regression-trained-above"><a href="#Example-2-Bayesian-regression-example-tuning-the-houses-price-regression-trained-above" class="headerlink" title="Example 2. Bayesian regression example - tuning the houses price regression trained above"></a>Example 2. Bayesian regression example - tuning the houses price regression trained above</h2><p>Now if say we have the results of the previos estimate (but not retain the training data anymore) and later find more training data to consider, we can further adjust the former estimate through the above Bayesian framework.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#consider the remaining part of the housing prices data separately</span></span><br><span class="line">REsample12=REsample1.loc[<span class="number">100</span>:]</span><br><span class="line"><span class="comment">#and compute the OLS estimate</span></span><br><span class="line">RE12w0=sum(REsample12.sale_price*REsample12.gross_sq_feet)/sum(REsample12.gross_sq_feet**<span class="number">2</span>)</span><br><span class="line">print(<span class="string">'Estimated w=&#123;0&#125;'</span>.format(RE12w0))</span><br></pre></td></tr></table></figure>
<pre><code>Estimated w=450.921318897
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#now implement Bayesian regression starting from the prior obtained based on the first sample w ~ N(369.713904612,18.293523802)</span></span><br><span class="line">RE12w0=sum(REsample12.sale_price*REsample12.gross_sq_feet)/sum(REsample12.gross_sq_feet**<span class="number">2</span>)</span><br><span class="line">s22=sigma**<span class="number">-2</span>; s12=RE11s**<span class="number">-2</span></span><br><span class="line">RE12w=(s22*sum(REsample12.sale_price*REsample12.gross_sq_feet)+s12*RE11w)/(s22*sum(REsample12.gross_sq_feet**<span class="number">2</span>)+s12)</span><br><span class="line">RE12s=(s22*sum(REsample12.gross_sq_feet**<span class="number">2</span>)+s12)**(<span class="number">-0.5</span>)</span><br><span class="line">print(<span class="string">'w ~ N(&#123;0&#125;,&#123;1&#125;)'</span>.format(RE12w,RE12s))</span><br></pre></td></tr></table></figure>
<pre><code>w ~ N(423.262774919,10.6761381647)
</code></pre><p>This is already pretty consistend with what python will give us if we take the entire training sample at once</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lm = smf.ols(formula=<span class="string">'sale_price ~ gross_sq_feet -1'</span>, data = REsample1).fit()</span><br><span class="line">print(lm.summary())</span><br></pre></td></tr></table></figure>
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:             sale_price   R-squared:                       0.835
Model:                            OLS   Adj. R-squared:                  0.835
Method:                 Least Squares   F-statistic:                     1562.
Date:                Mon, 01 Feb 2016   Prob (F-statistic):          1.13e-122
Time:                        23:30:24   Log-Likelihood:                -4414.8
No. Observations:                 309   AIC:                             8832.
Df Residuals:                     308   BIC:                             8835.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
---------------------------------------------------------------------------------
gross_sq_feet   423.2628     10.708     39.528      0.000       402.193   444.332
==============================================================================
Omnibus:                      208.010   Durbin-Watson:                   1.811
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3821.940
Skew:                           2.438   Prob(JB):                         0.00
Kurtosis:                      19.525   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre><p>So Bayesian framework allows to efficiently utilize prior knowledge on the regression parameters to be estimated and also provides us not just with one single best estimate for them but with the entire distribution for the possible value, admitting uncertainty. </p>
<p>Let’s also give some intuition of how strongly the prior knowledge affects the result compared to the new knowledge obtained from the training sample. Clearly if the prior knowledge is obtained from just a few observations while the further sample considered is pretty big, the result should be dominated by the further observations with little impact of the prior. But it seems that there is no information about how big the initial sample was… However this is not actually the case! </p>
<p>The key here is the deviation we have in the prior distribution - the fewer data was used to generate the prior estimate, the more uncertain it will be (higher deviation) and the smaller will be its impact on the posterior. For example uninformed prior having variance going to infinity, tends to have no impact on the result. On the other hand, a very narrow prior means that it has been obtained based on a very strong evidence and should have a major impact on the posterior despite further observations. So all necessary information is already in the prior - when we combine prior estimate based on one sample and with the further observations from the second sample, the first sample size is somehow “encoded” in the prior variance and all the necessary information is already there.</p>
<h1 id="Multivariate-case-and-regularization"><a href="#Multivariate-case-and-regularization" class="headerlink" title="Multivariate case and regularization"></a>Multivariate case and regularization</h1><p>Now consider a multivatiate linear regression model<br>$$<br>y\sim {\cal N}(w^T x,\sigma^2).<br>$$<br>Again assume that $\sigma$ is known or independent on $w$, while for the components of the vector $w=(w_1,w_2,…w_n)$ the prior distributions are known to be pairwise independent normal distributions<br>$$<br>w_j\sim{\cal N}(w_j^<em>,\sigma_j^</em>).<br>$$</p>
<p>$$<br>p(w|Y,X)\sim p(Y|w,X)p(w)=p(w)\prod\limits_i p(y=y_i|w,x=x_i)p(w)=\prod_i p(y=y_i|w,x=x_i)\prod_j p(w_j)\sim<br>$$<br>$$<br>\sim\prod\limits_i e^{\frac{-(y_i-w^T x_i)^2}{2\sigma^2}}\prod\limits_j e^{-\frac{(w_j-w_j^<em>)^2}{2(\sigma_i^</em>)^2}}=e^{-\frac{RSS(w)}{2\sigma^2}-\sum\limits_j \frac{(w_j-w_j^<em>)^2}{2(\sigma_i^</em>)^2}},<br>$$</p>
<p>so the estimate for the best fit $\hat{w}$ could be found from<br>$$<br>\hat{w}=argmax_w p(w|Y, X)=argmin_w\left[\frac{RSS(w)}{\sigma^2}+\sum\limits_j\frac{(w_j-w_j^<em>)^2}{(\sigma_j^</em>)^2}\right]\hspace{5ex}<br>$$</p>
<p>An important particular case of all the components of $w$ having the same prior $w_j\sim{\cal N}(0,\sigma/\sqrt{\lambda})$ leads to</p>
<p>$$<br>\hat{w}=argmin_w\left[RSS(w)+\lambda||w||_2^2\right]\hspace{5ex}(3)<br>$$</p>
<p>where $||w||_2=\sqrt{\sum\limits_j w_j^2}$, known as Ridge regression.</p>
<p>An alternative choice of the Laplacian prior distribution $p(w_j)\sim e^{-\lambda|w_j|/\sigma}$ leads to<br>$$<br>\hat{w}=argmin_w\left[RSS(w)+\lambda||w||_1\right],\hspace{5ex}(4)<br>$$<br>where $||w||_1=\sum\limits_j |w_j|$, known as Lasso (least absolute shrinkage and selection operator) regression.</p>
<p>Both Ridge and Lasso could be shown to be equivalent to a constrained minimization of $RSS$:</p>
<p>$$<br>RSS(w)\to min, \ ||w||_p\leq \alpha,\hspace{5ex}(5)<br>$$</p>
<p>with $p=1,2$ respectively, although analytic relation between constants $\alpha$ and $\lambda$ is somewhat nontrivial. In practice however the choice of $\lambda$ or $\alpha$ is usually empirical anyway, so both regularized (i.e. (3)-(4) with adding a regularization term ||w||) or constrained forms (5) of the optimization problem are equally applicable.</p>
<p>Optimization problems (3) and (4) tend to minimize $RSS$ at the same time penalizing the regression for having $||w||$ too large (regularization) which often leads to the model complexity through multiple regressors with large coefficients canceling effect of each other. So in a sense Lasso and Ridge are trying to avoid this situation, looking for relatively simple “regular” models with best possible fit. </p>
<p>Their Bayesian derivation show that Lasso and Ridge simply perform the regression with the prior belief that all the components of the $w$ are limited through the fixed variance of the priors. Such a belief affects the final outcome of the model making solutions with large $||w||$ to be particularly unlikely.</p>
<p>This helps Ridge and Lasso to fight overfitting also dealing with multicollinearity of regressors to some extent, preventing from learning noise through particularly complex “unnatural” combinations of the regressors.</p>
<p>Ridge regression admits solution in the closed form (consider partial derivatives of the objective function with respect to $w_j$):</p>
<p>$$<br>\hat{w}=(X^T X+\lambda I)^{-1}X^T Y, \hspace{5ex} (6)<br>$$</p>
<p>where $I$ is the identity $n\times n$ matrix, while $n$ being the number of regressors. The formulae (6) shows that the Ridge regression can in theory deal with the case of multicollinearity, when the matrix $X^T X$ is singular and OLS estimate does not exist.</p>
<p>Lasso does not admit solution in the closed form and requires numerical methods (like subgradient methods) to be fit.<br>Lasso however has an advantage of being able to completely eliminate impact of certain irrelevant regressors setting the corresponding slope coefficients to zero.  </p>
<h1 id="Bias-variance-trade-off"><a href="#Bias-variance-trade-off" class="headerlink" title="Bias-variance trade-off"></a>Bias-variance trade-off</h1><p>Suppose that the data considered have been indeed generated by a certain “true” model<br>$$<br>y\sim {\cal N}(w^T x,\sigma^2),<br>$$<br>while “true” $w$ is not known to us and is being approximated by our estimates $\hat{w}$ picked up from the posterior distribution given by our model (e.g. OLS, Ridge, Lasso etc).</p>
<p>The ability of the model to predict future values of $y$ for specific $x$ can be expressed as (using $E[X^2]=E[X]^2+Var[X]$ and $E[y]=E[w^T x]$):</p>
<p>$$<br>E\left[(y-\hat{w}^T x)^2\right]=Bias[\hat{w}^T x]^2+Var[\hat{w}^T x]^2+\sigma^2,<br>$$</p>
<p>where for each specific $x$<br>$$<br>{\rm Bias}[\hat{w}^T x]=(E[\hat{w}^T x]-w^T x)<br>$$<br>shows how shifted on average will our prediction be vs the unknown “true” model prediction, while<br>$$<br>Var[\hat{w}^T x]=E[(\hat{w}^T x-E[\hat{w}^T x])^2]<br>$$<br>shows how uncertain is our prediction given the variation of the estimate $\hat{w}$. The third term $\sigma^2$ corresponds to an irreducible error, caused by the noise in the data generation process.</p>
<p>So the ability of the model to predict future values of $y$ depends (besides the irreducible error which we’re not in control of) on both - the variance in our estimae of $\hat{w}$ as well as the bias in our estimation (i.e. systematic shift like under or overestimation) of the true values of $w$ and $y$. </p>
<p>The OLS estimate in theory is known to be unbiased (tends to have zero bias provided that the training sample is large enough), however might have pretty high variance in $\hat{w}$ especially when the regressors are highly correlated. This reduces its generalizeability. Regularized regression, like Ridge and Lasso can introduce some bias (might not fit the training sample that well), but may largely reduce variance by limiting the $||w||$ (and become much less sensitive to small changes in the training data, reducing the risk of overfitting).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line">Image(filename=<span class="string">'data/biasvariance.png'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="NBsession2_files/NBsession2_32_0.png" alt="png"></p>
<p>Generally, often the more complex model able to better learn from the training sample (lower bias) have higher variance, which leads to overfitting and high error for predicting the future values of the output variable.</p>
<h3 id="Validation-and-cross-validation"><a href="#Validation-and-cross-validation" class="headerlink" title="Validation and cross-validation"></a>Validation and cross-validation</h3><p>A good way to validate the model is to use a test set separate from the training sample to evaluate the model performance. Additionally if the model depends on the certain parameters (like $\alpha,\lambda$ for Lasso/Ridge) which are not supposed to be fit during the training phase, a separate validation sample could be used for the selection of model parameters (we pick up those which optimize model performance over the validation set). Usually test and validation sets are got as subsamples (often random) of the available dataset, while the remaining data is used as the training sample. </p>
<p>But often the available dataset is small enough, so splitting into into even smaller traning, validation and test sets could have negative impact on the model training leading to noisy and unreliable models. In such cases cross-validation is often applied, performing not one but several random splits of the sample with further averaging of the model performance scores. </p>
<h2 id="Choice-of-lambda-or-alpha-Model-validation"><a href="#Choice-of-lambda-or-alpha-Model-validation" class="headerlink" title="Choice of $\lambda$ or $\alpha$. Model validation."></a>Choice of $\lambda$ or $\alpha$. Model validation.</h2><p>In both - constrained and regularized forms - the parameter $\alpha$ or $\lambda$ of Lasso/Ridge regression is somewhat arbitrary and can take any value from $0$ to $+\infty$. Small values of $\lambda$ or high values of $\alpha$ lead to the result close to OLS (identical to it is $\lambda=0$ or $\alpha=+\infty$), while large $\lambda$ or small $\alpha$ tend to overemphasize the impact of regularization over the fit itself. There is no single best way of choosing the value of the regularization parameter - what is usually suggested is to fit it by evaluating the model for different values of the parameter over the separate validation set and picking up the value for which the validation performance is the best one. As for the performance metric one can use RSS or, equivalently, R2.</p>
<h2 id="Example-3-Regularization-with-artifical-data"><a href="#Example-3-Regularization-with-artifical-data" class="headerlink" title="Example 3. Regularization with artifical data"></a>Example 3. Regularization with artifical data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Upload the data</span></span><br><span class="line">data3=pd.read_csv(<span class="string">"data/example3.csv"</span>) <span class="comment"># load the data </span></span><br><span class="line">y=np.asarray(data3.iloc[:,<span class="number">-1</span>]) <span class="comment"># make dependent variable</span></span><br><span class="line">X=np.asarray(data3.iloc[:,<span class="number">0</span>:<span class="number">-1</span>]) <span class="comment">#make independent variables</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(len(data3))</span><br><span class="line">data3.head()</span><br></pre></td></tr></table></figure>
<pre><code>50
</code></pre><div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>0</th><br>      <th>1</th><br>      <th>2</th><br>      <th>3</th><br>      <th>4</th><br>      <th>5</th><br>      <th>6</th><br>      <th>7</th><br>      <th>8</th><br>      <th>9</th><br>      <th>…</th><br>      <th>20</th><br>      <th>21</th><br>      <th>22</th><br>      <th>23</th><br>      <th>24</th><br>      <th>25</th><br>      <th>26</th><br>      <th>27</th><br>      <th>28</th><br>      <th>Y</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>26</td><br>      <td>22.415927</td><br>      <td>1.481339</td><br>      <td>9.634145</td><br>      <td>-15.316976</td><br>      <td>3.707672</td><br>      <td>44.960785</td><br>      <td>27.480018</td><br>      <td>48.835505</td><br>      <td>5.382629</td><br>      <td>…</td><br>      <td>42.863416</td><br>      <td>39</td><br>      <td>26</td><br>      <td>95</td><br>      <td>83</td><br>      <td>16</td><br>      <td>21</td><br>      <td>66</td><br>      <td>87</td><br>      <td>17.023843</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>68</td><br>      <td>59.433665</td><br>      <td>14.979972</td><br>      <td>22.192560</td><br>      <td>-35.872848</td><br>      <td>12.682355</td><br>      <td>117.701114</td><br>      <td>66.681792</td><br>      <td>124.983058</td><br>      <td>18.150923</td><br>      <td>…</td><br>      <td>101.877161</td><br>      <td>45</td><br>      <td>52</td><br>      <td>24</td><br>      <td>42</td><br>      <td>97</td><br>      <td>65</td><br>      <td>47</td><br>      <td>94</td><br>      <td>73.075316</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>45</td><br>      <td>39.162046</td><br>      <td>7.587863</td><br>      <td>15.315333</td><br>      <td>-24.616061</td><br>      <td>7.767647</td><br>      <td>77.867124</td><br>      <td>45.214154</td><br>      <td>83.283207</td><br>      <td>11.158762</td><br>      <td>…</td><br>      <td>69.560110</td><br>      <td>53</td><br>      <td>64</td><br>      <td>31</td><br>      <td>31</td><br>      <td>64</td><br>      <td>74</td><br>      <td>19</td><br>      <td>55</td><br>      <td>34.872741</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>91</td><br>      <td>79.705283</td><br>      <td>22.372081</td><br>      <td>29.069787</td><br>      <td>-47.129635</td><br>      <td>17.597062</td><br>      <td>157.535104</td><br>      <td>88.149430</td><br>      <td>166.682908</td><br>      <td>25.143084</td><br>      <td>…</td><br>      <td>134.194211</td><br>      <td>90</td><br>      <td>50</td><br>      <td>49</td><br>      <td>30</td><br>      <td>95</td><br>      <td>65</td><br>      <td>80</td><br>      <td>77</td><br>      <td>64.479635</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>15</td><br>      <td>12.720805</td><br>      <td>-2.054018</td><br>      <td>6.345036</td><br>      <td>-9.933295</td><br>      <td>1.357160</td><br>      <td>25.909746</td><br>      <td>17.212886</td><br>      <td>28.892098</td><br>      <td>2.038551</td><br>      <td>…</td><br>      <td>27.407435</td><br>      <td>9</td><br>      <td>49</td><br>      <td>73</td><br>      <td>23</td><br>      <td>76</td><br>      <td>37</td><br>      <td>44</td><br>      <td>55</td><br>      <td>19.937208</td><br>    </tr><br>  </tbody><br></table><br><p>5 rows × 30 columns</p><br></div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Let's have a quick look of the OLS for whole data</span></span><br><span class="line">result=ols(y=y,x=pd.DataFrame(X))</span><br><span class="line"><span class="comment"># You will see NaN values becuase the number is too small or too big to calculate here. We do not need to care about it</span></span><br><span class="line"><span class="comment">#since we only care about prediction. The coeffiencts might be unreliable due to the collinearity. </span></span><br><span class="line">result.summary_as_matrix.T</span><br></pre></td></tr></table></figure>
<div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>beta</th><br>      <th>p-value</th><br>      <th>std err</th><br>      <th>t-stat</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>3.757231e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>-7.496352e+07</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>9.353953e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>2.537093e+09</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>-4.952650e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>5</th><br>      <td>8.663074e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>6</th><br>      <td>1.194379e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>7</th><br>      <td>-5.983284e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>8</th><br>      <td>1.651503e+07</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>9</th><br>      <td>-2.162900e+09</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>10</th><br>      <td>3.144394e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>11</th><br>      <td>-5.027232e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>12</th><br>      <td>-5.629380e+07</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>13</th><br>      <td>-2.188585e+09</td><br>      <td>1.108756e-24</td><br>      <td>3.393004e+07</td><br>      <td>-64.502884</td><br>    </tr><br>    <tr><br>      <th>14</th><br>      <td>1.239714e+09</td><br>      <td>1.290337e-34</td><br>      <td>6.111422e+06</td><br>      <td>202.851889</td><br>    </tr><br>    <tr><br>      <th>15</th><br>      <td>-7.542769e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>16</th><br>      <td>-6.820656e+09</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>17</th><br>      <td>5.683494e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>18</th><br>      <td>1.267237e+08</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>19</th><br>      <td>1.923137e+09</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>20</th><br>      <td>-2.540981e+07</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>21</th><br>      <td>-7.250141e-02</td><br>      <td>2.006087e-01</td><br>      <td>5.478117e-02</td><br>      <td>-1.323473</td><br>    </tr><br>    <tr><br>      <th>22</th><br>      <td>1.007564e-02</td><br>      <td>8.666941e-01</td><br>      <td>5.925773e-02</td><br>      <td>0.170031</td><br>    </tr><br>    <tr><br>      <th>23</th><br>      <td>-4.209827e-03</td><br>      <td>9.325065e-01</td><br>      <td>4.908597e-02</td><br>      <td>-0.085764</td><br>    </tr><br>    <tr><br>      <th>24</th><br>      <td>1.369836e-01</td><br>      <td>1.835671e-02</td><br>      <td>5.334506e-02</td><br>      <td>2.567877</td><br>    </tr><br>    <tr><br>      <th>25</th><br>      <td>3.884839e-02</td><br>      <td>4.017734e-01</td><br>      <td>4.534793e-02</td><br>      <td>0.856674</td><br>    </tr><br>    <tr><br>      <th>26</th><br>      <td>-5.104671e-03</td><br>      <td>9.203430e-01</td><br>      <td>5.040557e-02</td><br>      <td>-0.101272</td><br>    </tr><br>    <tr><br>      <th>27</th><br>      <td>-1.539899e-01</td><br>      <td>7.958639e-03</td><br>      <td>5.224136e-02</td><br>      <td>-2.947662</td><br>    </tr><br>    <tr><br>      <th>28</th><br>      <td>-1.093492e-01</td><br>      <td>4.670594e-02</td><br>      <td>5.157943e-02</td><br>      <td>-2.120016</td><br>    </tr><br>    <tr><br>      <th>intercept</th><br>      <td>-1.641190e+10</td><br>      <td>8.018373e-25</td><br>      <td>2.503286e+08</td><br>      <td>-65.561425</td><br>    </tr><br>  </tbody><br></table><br></div>



<p>Let’s evaluate if this OLS model for such a dataset is actually generalizeable. For that purpose let us divide the dataset into training set and the test set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_index=[<span class="number">22</span>,<span class="number">15</span>,<span class="number">31</span>,<span class="number">18</span>,<span class="number">34</span>,<span class="number">32</span>,<span class="number">12</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">0</span>,<span class="number">44</span>,<span class="number">5</span>,<span class="number">41</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">21</span>,<span class="number">11</span>,<span class="number">28</span>,<span class="number">14</span>,<span class="number">46</span>,<span class="number">8</span>,<span class="number">25</span>,<span class="number">20</span>,<span class="number">33</span>,<span class="number">35</span>,<span class="number">39</span>,<span class="number">10</span>,<span class="number">27</span>,<span class="number">7</span>,<span class="number">42</span>]</span><br><span class="line">test_index=[x <span class="keyword">for</span> x <span class="keyword">in</span> list(range(len(X))) <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> train_index]</span><br><span class="line"><span class="comment">#Generate data for training and testing.</span></span><br><span class="line">X_train=X[train_index,:] <span class="comment">#training variables</span></span><br><span class="line">y_train=y[train_index] <span class="comment">#training dependent variables</span></span><br><span class="line">X_test=X[test_index,:] <span class="comment">#prediction variables</span></span><br><span class="line">y_test=y[test_index] <span class="comment">#prediction dependent variable</span></span><br><span class="line">print(<span class="string">'Size of the training set=&#123;0&#125;, test set=&#123;1&#125;'</span>).format(len(train_index),len(test_index))</span><br></pre></td></tr></table></figure>
<pre><code>Size of the training set=30, test set=20
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In the sample. (Only use training set)</span></span><br><span class="line">result=ols(y=y_train,x=pd.DataFrame(X_train))</span><br><span class="line"><span class="comment">#print(result.summary_as_matrix.T)</span></span><br><span class="line">R_2_IS=result.r2  <span class="comment"># get R2</span></span><br><span class="line">OLS_coef=result.beta</span><br><span class="line"></span><br><span class="line"><span class="comment">#Out of sample </span></span><br><span class="line">a=np.array(X_test)  <span class="comment">#makes sure conver pd data to np array</span></span><br><span class="line">b=np.array(result.beta) <span class="comment">#makes sure conver pd data to np array</span></span><br><span class="line">print(<span class="string">'OLS regression coefficients=&#123;0&#125;'</span>.format(b))</span><br><span class="line">c=np.sum(a*b[<span class="number">0</span>:<span class="number">-1</span>],axis=<span class="number">1</span>)+b[<span class="number">-1</span>] <span class="comment">#b is estimated coefficients, a is prediction data, b[-1] is intercept. This is for predicted y</span></span><br><span class="line">error=y_test-c <span class="comment"># y_predict is real value, c is the value we guessed</span></span><br><span class="line">R_2_OS=<span class="number">1</span>-error.var()/y_test.var() <span class="comment"># this is out of sample R2</span></span><br><span class="line">print(<span class="string">"The R-squared we found for in-sample (IS) OLS is: &#123;0&#125;"</span>.format(R_2_IS))</span><br><span class="line">print(<span class="string">"The R-squared we found for out-of-sample (OS) OLS is: &#123;0&#125;"</span>.format(R_2_OS))</span><br></pre></td></tr></table></figure>
<pre><code>OLS regression coefficients=[  2.96536426e+10   1.44001991e+10  -2.31281699e+10  -9.13020363e+09
  -1.41158081e+10   1.15295408e+10   1.30825936e+09  -5.53322155e+09
  -1.42820790e+09  -4.91641257e+10   1.03908016e+10   4.46879158e+09
   7.63373753e+08   1.79675569e+11  -8.06002813e+09   1.02906034e+09
  -9.60511890e+10   1.55159832e+09  -1.83746940e+09  -6.59797711e+08
  -2.89938243e+09  -2.01922977e+00  -1.09512337e+00  -4.34008447e-01
   1.24791467e+00  -2.51666116e-01   1.71862102e+00  -9.25140181e-01
  -9.23786023e-01  -1.70819989e+12]
The R-squared we found for in-sample (IS) OLS is: 0.999999965428
The R-squared we found for out-of-sample (OS) OLS is: -82.7531318928
</code></pre><p>So while OLS reached almost perfect IS performance over the training set (which is not too suprizing as one would expect 29 regressors to be able to learn nearly all the infromation from the sample of 30 observations) it’s generalizeability is quite poor (OS R2 goes way below zero). So there is clearly an overfitting problem. Let’s track its dynamics adding variables to the regression one by one.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#OS OLS R2 deneding on the number of variables t</span></span><br><span class="line"></span><br><span class="line">Number_variables=[]</span><br><span class="line"></span><br><span class="line">OLS_R_2_OS_F=[]</span><br><span class="line">OLS_R_2_IS_F=[]</span><br><span class="line"></span><br><span class="line">t=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################################################################</span></span><br><span class="line"><span class="comment">##########################################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(len(X_train.T)): <span class="comment">#subsequently add variables</span></span><br><span class="line">    </span><br><span class="line">    t+=<span class="number">1</span></span><br><span class="line">    Number_variables.append(t)</span><br><span class="line"></span><br><span class="line">    result=ols(y=y_train,x=pd.DataFrame(X_train[:,<span class="number">0</span>:j+<span class="number">1</span>]))</span><br><span class="line">    temp=X_test[:,<span class="number">0</span>:j+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    a=np.array(temp)</span><br><span class="line">    b=np.array(result.beta)</span><br><span class="line">    c=np.sum(a*b[<span class="number">0</span>:<span class="number">-1</span>],axis=<span class="number">1</span>)+b[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    error=y_test-c</span><br><span class="line">    R_2=<span class="number">1</span>-error.var()/y_test.var()</span><br><span class="line">    <span class="keyword">if</span> R_2&gt;<span class="number">0</span>:</span><br><span class="line">        OLS_R_2_OS_F.append(R_2)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        OLS_R_2_OS_F.append(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    OLS_R_2_IS_F.append(result.r2)</span><br><span class="line"></span><br><span class="line">pylab.title(<span class="string">'OS performance of OLS when we subsequently add variables'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_OS_F,<span class="string">'b'</span>,label=<span class="string">'R_squared_OLS_OS'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_IS_F,<span class="string">'g'</span>,label=<span class="string">'R_squared_OLS_IS'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pylab.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">pylab.xlabel(<span class="string">'Number of independent variables'</span>)</span><br><span class="line">pylab.ylabel(<span class="string">'R-squared'</span>)</span><br><span class="line">pylab.legend(loc=<span class="string">'lower left'</span>)</span><br><span class="line">pylab.show()</span><br></pre></td></tr></table></figure>
<p><img src="NBsession2_files/NBsession2_43_0.png" alt="png"></p>
<p>As we see OS OLS R2 is pretty different for IS R2 and the difference starts to grow especially rapidly for $t&gt;20$, so there is clearly a major overfitting problem with OLS at that point. Let’s see if Lasso/Ridge are able to fix this issue.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Ridge=linear_model.Ridge(fit_intercept=<span class="keyword">True</span>,alpha=<span class="number">1</span>) <span class="comment">#try Ridge with an arbitrary regularization parameter lambda=1</span></span><br><span class="line"></span><br><span class="line">Ridge.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># In the sample:</span></span><br><span class="line">p_IS=Ridge.predict(X_train)</span><br><span class="line">err_IS=p_IS-y_train</span><br><span class="line">R_2_IS_Ridge=<span class="number">1</span>-np.var(err_IS)/np.var(y_train)</span><br><span class="line">print(<span class="string">"The R-squared we found for IS Ridge is: &#123;0&#125;"</span>.format(R_2_IS_Ridge))</span><br><span class="line"></span><br><span class="line">Ridge_coef=Ridge.coef_</span><br><span class="line"><span class="comment">############################################################################    </span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#Out of sample</span></span><br><span class="line">p_OS=Ridge.predict(X_test)</span><br><span class="line">err_OS=p_OS-y_test</span><br><span class="line">R_2_OS_Ridge=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line">print(<span class="string">"The R-squared we found for OS Ridge is: &#123;0&#125;"</span>.format(R_2_OS_Ridge))</span><br></pre></td></tr></table></figure>
<pre><code>The R-squared we found for IS Ridge is: 0.835638878661
The R-squared we found for OS Ridge is: 0.615809865692
</code></pre><p>So even with a pretty arbitrary choice of lambda Ridge works pretty well, mostly fixing the overfitting issue. Let’s see if one can do better with a more sophisticated choice - for that purpose further split the former training set into training and validation, so that for each lambda the model is being trained over the smaller training sample and optimal lambda is selected based on the model performance over the validation set. Then final result is evaluated over the test set as before.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Find the Alpha and report best test performance for Ridge/Lasso.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Regularization_fit_lambda</span><span class="params">(model,X_train,y_train,lambdas,p=<span class="number">0.4</span>,Graph=False, logl=False)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#model=1 -Ridge, 2-Lasso</span></span><br><span class="line">    <span class="comment">#X_train, y_train is training set</span></span><br><span class="line">    <span class="comment">#lambdas: a list of lambda values to try</span></span><br><span class="line">    <span class="comment">#p: ratio of the validation sample size/ total training size</span></span><br><span class="line">    <span class="comment">#Graph: plot the graph of R^2 values for different lambda</span></span><br><span class="line"><span class="comment">##############################################################################################################################    </span></span><br><span class="line">    </span><br><span class="line">    rnd.seed(<span class="number">2014</span>)</span><br><span class="line">    R_2_OS=[] <span class="comment">#make a list to store R-squared. We need in end to pick the lambda having the biggest R2.</span></span><br><span class="line">    </span><br><span class="line">    validation_index=rnd.sample(list(range(len(X_train))),int(len(X_train)*p) ) <span class="comment">#choose some rows for the validation sample    </span></span><br><span class="line">    training_index=[x <span class="keyword">for</span> x <span class="keyword">in</span> list(range(len(X_train))) <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> validation_index] <span class="comment"># Use the rest of rows as a training sample</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    X_validation=X_train[validation_index,:]</span><br><span class="line">    y_validation=y_train[validation_index]</span><br><span class="line"></span><br><span class="line">    X_train0=X_train[training_index,:]</span><br><span class="line">    y_train0=y_train[training_index]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> lambdas:</span><br><span class="line">        <span class="keyword">if</span> model==<span class="number">1</span>:</span><br><span class="line">            RM=linear_model.Ridge(fit_intercept=<span class="keyword">True</span>, alpha=a)</span><br><span class="line">            model_label=<span class="string">'Ridge'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            RM=linear_model.Lasso(fit_intercept=<span class="keyword">True</span>, alpha=a)</span><br><span class="line">            model_label=<span class="string">'Lasso'</span></span><br><span class="line">        RM.fit(X_train0,y_train0)  <span class="comment">#fit the regularization model</span></span><br><span class="line">        </span><br><span class="line">        y_predict=RM.predict(X_validation) <span class="comment">#compute the prediction for the validation sample </span></span><br><span class="line">        err_OS=y_predict-y_validation</span><br><span class="line">        R_2_OS_=<span class="number">1</span>-np.var(err_OS)/np.var(y_validation)</span><br><span class="line">        R_2_OS.append(R_2_OS_)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Find the alpha that c.r.t the biggest R^2</span></span><br><span class="line">    best_R2 = max(R_2_OS)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(R_2_OS)):</span><br><span class="line">        <span class="keyword">if</span> R_2_OS[i]==best_R2:</span><br><span class="line">            best_lambda=lambdas[i]</span><br><span class="line">    <span class="comment">#print("The alpha we found for Ridge is:&#123;0&#125;".format(l))</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Graph</span></span><br><span class="line">    <span class="keyword">if</span> Graph==<span class="keyword">True</span>:</span><br><span class="line">        pylab.title(<span class="string">'IS R-squared vs OS-R-squared for different Lambda'</span>)</span><br><span class="line">        <span class="keyword">if</span> logl:</span><br><span class="line">            pylab.xlabel(<span class="string">'ln(Lambda)'</span>)</span><br><span class="line">            l=log(lambdas)</span><br><span class="line">            bl=log(best_lambda)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pylab.xlabel(<span class="string">'Lambda'</span>)</span><br><span class="line">            l=lambdas</span><br><span class="line">            bl=best_lambda</span><br><span class="line">        pylab.plot(l,R_2_OS,<span class="string">'b'</span>,label=model_label)</span><br><span class="line">        pylab.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">        pylab.ylabel(<span class="string">'R-squared'</span>)</span><br><span class="line">        pylab.axvline(bl,color=<span class="string">'r'</span>,linestyle=<span class="string">'--'</span>)</span><br><span class="line"></span><br><span class="line">        pylab.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_lambda</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#select best lambda for Ridge</span></span><br><span class="line">lambdas = np.linspace(<span class="number">-5</span>,<span class="number">13</span>,<span class="number">200</span>)</span><br><span class="line">lambdas=[math.exp(i) <span class="keyword">for</span> i <span class="keyword">in</span> lambdas]</span><br><span class="line">lambda_r_optimal=Regularization_fit_lambda(<span class="number">1</span>,X_train,y_train,lambdas,p=<span class="number">0.4</span>,Graph=<span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">'Optimal lambda for Ridge=&#123;0&#125;'</span>.format(lambda_r_optimal))</span><br></pre></td></tr></table></figure>
<p><img src="NBsession2_files/NBsession2_48_0.png" alt="png"></p>
<pre><code>Optimal lambda for Ridge=55248.5839921
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Ridge=linear_model.Ridge(fit_intercept=<span class="keyword">True</span>,alpha=lambda_r_optimal) <span class="comment">#try Ridge with a selected regularization parameter lambda</span></span><br><span class="line"></span><br><span class="line">Ridge.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># In the sample:</span></span><br><span class="line">p_IS=Ridge.predict(X_train)</span><br><span class="line">err_IS=p_IS-y_train</span><br><span class="line">R_2_IS_Ridge=<span class="number">1</span>-np.var(err_IS)/np.var(y_train)</span><br><span class="line">print(<span class="string">"The R-squared we found for IS Ridge is: &#123;0&#125;"</span>.format(R_2_IS_Ridge))</span><br><span class="line"></span><br><span class="line">Ridge_coef=Ridge.coef_</span><br><span class="line"><span class="comment">############################################################################    </span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#Out of sample</span></span><br><span class="line">p_OS=Ridge.predict(X_test)</span><br><span class="line">err_OS=p_OS-y_test</span><br><span class="line">R_2_OS_Ridge=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line">print(<span class="string">"The R-squared we found for OS Ridge is: &#123;0&#125;"</span>.format(R_2_OS_Ridge))</span><br></pre></td></tr></table></figure>
<pre><code>The R-squared we found for IS Ridge is: 0.815602153898
The R-squared we found for OS Ridge is: 0.632932481545
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ridge_coef</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.02517172,  0.02218572,  0.00809009,  0.00752659, -0.01231968,
        0.00537877,  0.04359522,  0.02349467,  0.04563726,  0.00765238,
       -0.01838121,  0.00980197, -0.03850794, -0.00288114, -0.00891382,
        0.02357866, -0.00111899, -0.0097049 , -0.04449715,  0.00775819,
        0.03536851, -0.00568311,  0.0124728 , -0.00536527, -0.01624446,
       -0.00551884,  0.01745864, -0.0198678 ,  0.00324518])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#select lambdas for Lasso</span></span><br><span class="line">lambdas = np.linspace(<span class="number">-5</span>,<span class="number">6.5</span>,<span class="number">200</span>)</span><br><span class="line">lambdas=[math.exp(i) <span class="keyword">for</span> i <span class="keyword">in</span> lambdas]</span><br><span class="line">lambda_l_optimal=Regularization_fit_lambda(<span class="number">2</span>,X_train,y_train,lambdas,p=<span class="number">0.4</span>,Graph=<span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">'Optimal lambda for Lasso=&#123;0&#125;'</span>.format(lambda_l_optimal))</span><br></pre></td></tr></table></figure>
<pre><code>/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:444: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations
  ConvergenceWarning)
</code></pre><p><img src="NBsession2_files/NBsession2_51_1.png" alt="png"></p>
<pre><code>Optimal lambda for Lasso=83.0628865303
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Lasso=linear_model.Lasso(fit_intercept=<span class="keyword">True</span>,alpha=lambda_l_optimal) <span class="comment">#try Ridge with a selected regularization parameter lambda</span></span><br><span class="line"></span><br><span class="line">Lasso.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># In the sample:</span></span><br><span class="line">p_IS=Lasso.predict(X_train)</span><br><span class="line">err_IS=p_IS-y_train</span><br><span class="line">R_2_IS_Lasso=<span class="number">1</span>-np.var(err_IS)/np.var(y_train)</span><br><span class="line">print(<span class="string">"The R-squared we found for IS Lasso is: &#123;0&#125;"</span>.format(R_2_IS_Ridge))</span><br><span class="line"></span><br><span class="line">Lasso_coef=Lasso.coef_</span><br><span class="line"><span class="comment">############################################################################    </span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#Out of sample</span></span><br><span class="line">p_OS=Lasso.predict(X_test)</span><br><span class="line">err_OS=p_OS-y_test</span><br><span class="line">R_2_OS_Lasso=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line">print(<span class="string">"The R-squared we found for OS Lasso is: &#123;0&#125;"</span>.format(R_2_OS_Lasso))</span><br></pre></td></tr></table></figure>
<pre><code>The R-squared we found for IS Lasso is: 0.815602153898
The R-squared we found for OS Lasso is: 0.652257395852
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Lasso_coef</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.        ,  0.        ,  0.        ,  0.        , -0.        ,
        0.        ,  0.        ,  0.        ,  0.26033553,  0.        ,
       -0.        ,  0.        , -0.        , -0.        , -0.        ,
        0.        , -0.        , -0.        , -0.        ,  0.        ,
        0.        , -0.        ,  0.        , -0.        , -0.        ,
       -0.        ,  0.        , -0.        ,  0.        ])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#OS OLS R2 deneding on the number of variables t</span></span><br><span class="line"></span><br><span class="line">Number_variables=[]</span><br><span class="line"></span><br><span class="line">OLS_R_2_OS_F=[]</span><br><span class="line">OLS_R_2_IS_F=[]</span><br><span class="line">OLS_R_2_Ridge_OS_F=[]</span><br><span class="line">OLS_R_2_Ridge_IS_F=[]</span><br><span class="line">OLS_R_2_Lasso_OS_F=[]</span><br><span class="line">OLS_R_2_Lasso_IS_F=[]</span><br><span class="line"></span><br><span class="line">t=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################################################################</span></span><br><span class="line"><span class="comment">##########################################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(len(X_train.T)): <span class="comment">#subsequently add variables</span></span><br><span class="line">    </span><br><span class="line">    t+=<span class="number">1</span></span><br><span class="line">    Number_variables.append(t)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#For OLS</span></span><br><span class="line"></span><br><span class="line">    result=ols(y=y_train,x=pd.DataFrame(X_train[:,<span class="number">0</span>:j+<span class="number">1</span>]))</span><br><span class="line">    temp=X_test[:,<span class="number">0</span>:j+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    a=np.array(temp)</span><br><span class="line">    b=np.array(result.beta)</span><br><span class="line">    c=np.sum(a*b[<span class="number">0</span>:<span class="number">-1</span>],axis=<span class="number">1</span>)+b[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    error=y_test-c</span><br><span class="line">    R_2_OS_OLS=<span class="number">1</span>-error.var()/y_test.var()</span><br><span class="line">    <span class="keyword">if</span> R_2_OS_OLS&gt;<span class="number">0</span>:</span><br><span class="line">        OLS_R_2_OS_F.append(R_2_OS_OLS)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        OLS_R_2_OS_F.append(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    R_2_IS_OLS=result.r2</span><br><span class="line">    OLS_R_2_IS_F.append(R_2_IS_OLS)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#For Ridge</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    Ridge=linear_model.Ridge(fit_intercept=<span class="keyword">True</span>,alpha=lambda_r_optimal)</span><br><span class="line">    Ridge.fit(X_train[:,<span class="number">0</span>:j+<span class="number">1</span>],y_train)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># In the sample:</span></span><br><span class="line">    p_IS=Ridge.predict(X_train[:,<span class="number">0</span>:j+<span class="number">1</span>])</span><br><span class="line">    err_IS=p_IS-y_train</span><br><span class="line">    R_2_IS_Ridge=<span class="number">1</span>-np.var(err_IS)/np.var(y_train)</span><br><span class="line">    OLS_R_2_Ridge_IS_F.append(R_2_IS_Ridge)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Out of sample</span></span><br><span class="line">    p_OS=Ridge.predict(X_test[:,<span class="number">0</span>:j+<span class="number">1</span>])</span><br><span class="line">    err_OS=p_OS-y_test</span><br><span class="line">    R_2_OS_Ridge=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> R_2_OS_Ridge&gt;<span class="number">0</span>:</span><br><span class="line">        OLS_R_2_Ridge_OS_F.append(R_2_OS_Ridge)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        OLS_R_2_Ridge_OS_F.append(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">#For Lasso</span></span><br><span class="line">    Lasso=linear_model.Lasso(fit_intercept=<span class="keyword">True</span>, alpha=lambda_l_optimal)</span><br><span class="line">    Lasso.fit(X_train[:,<span class="number">0</span>:j+<span class="number">1</span>],y_train)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#In the sample:</span></span><br><span class="line">    p_IS=Lasso.predict(X_train[:,<span class="number">0</span>:j+<span class="number">1</span>])</span><br><span class="line">    err_IS=p_IS-y_train</span><br><span class="line">    R_2_IS_Lasso=<span class="number">1</span>-np.var(err_IS)/np.var(y_train)</span><br><span class="line">    OLS_R_2_Lasso_IS_F.append(R_2_IS_Lasso)</span><br><span class="line">  </span><br><span class="line"> <span class="comment">###########################################################################    </span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#Out of sample</span></span><br><span class="line">    p_OS=Lasso.predict(X_test[:,<span class="number">0</span>:j+<span class="number">1</span>])</span><br><span class="line">    err_OS=p_OS-y_test</span><br><span class="line">    R_2_OS_Lasso=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line">    <span class="keyword">if</span> R_2_OS&gt;<span class="number">0</span>:</span><br><span class="line">        OLS_R_2_Lasso_OS_F.append(R_2_OS_Lasso)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        OLS_R_2_Lasso_OS_F.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">pylab.title(<span class="string">'OS performance of OLS when we subsequently add variables'</span>)</span><br><span class="line"></span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_IS_F,<span class="string">'g'</span>,label=<span class="string">'R_squared_OLS_IS'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_Lasso_IS_F,<span class="string">'y'</span>,label=<span class="string">'R_squared_Lasso_IS'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_Ridge_IS_F,<span class="string">'k'</span>,label=<span class="string">'R_squard Ridge_IS'</span>)</span><br><span class="line"></span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_OS_F,<span class="string">'b'</span>,label=<span class="string">'R_squared_OLS_OS'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_Lasso_OS_F,<span class="string">'c'</span>,label=<span class="string">'R_squared_Lasso_OS'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_Ridge_OS_F,<span class="string">'r'</span>,label=<span class="string">'R_squard Ridge_OS'</span>)</span><br><span class="line"></span><br><span class="line">pylab.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">pylab.xlabel(<span class="string">'Number of independent variables'</span>)</span><br><span class="line">pylab.ylabel(<span class="string">'R-squared'</span>)</span><br><span class="line">pylab.legend(loc=<span class="string">'lower left'</span>)</span><br><span class="line">pylab.show()</span><br></pre></td></tr></table></figure>
<p><img src="NBsession2_files/NBsession2_54_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#let's look at the final coefficients</span></span><br><span class="line">name=[<span class="string">"var_&#123;0&#125;"</span>.format(i+<span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(OLS_coef)<span class="number">-1</span>)] <span class="comment"># Generate name of rows/variables</span></span><br><span class="line"></span><br><span class="line">result=[name, np.ndarray.tolist(np.asarray(OLS_coef[<span class="number">0</span>:len(OLS_coef)<span class="number">-1</span>])),np.ndarray.tolist(Ridge_coef),</span><br><span class="line">        np.ndarray.tolist(Lasso_coef)] <span class="comment"># put all the estimations of different models together(it is same for OS and IS)</span></span><br><span class="line">temp=pd.DataFrame(result).T</span><br><span class="line">R2_IS=pd.DataFrame([<span class="string">'R2_IS'</span>,R_2_IS_OLS, R_2_IS_Ridge,R_2_IS_Lasso]).T <span class="comment"># paste the In sample R2 in the end of temp</span></span><br><span class="line">R2_OS=pd.DataFrame([<span class="string">'R2_OS'</span>,R_2_OS_OLS, R_2_OS_Ridge,R_2_OS_Lasso]).T <span class="comment"># paste the Out of sample R2 in the end of temp</span></span><br><span class="line"></span><br><span class="line">temp=temp.append(R2_IS)</span><br><span class="line">result=temp.append(R2_OS)</span><br><span class="line">result.columns=[<span class="string">''</span>,<span class="string">'OLS'</span>,<span class="string">'Ridge'</span>,<span class="string">'Lasso'</span>] <span class="comment"># Change the column name</span></span><br><span class="line">result.set_index(<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>OLS</th><br>      <th>Ridge</th><br>      <th>Lasso</th><br>    </tr><br>    <tr><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>var_1</th><br>      <td>2.965364e+10</td><br>      <td>0.02517172</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_2</th><br>      <td>1.44002e+10</td><br>      <td>0.02218572</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_3</th><br>      <td>-2.312817e+10</td><br>      <td>0.008090091</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_4</th><br>      <td>-9.130204e+09</td><br>      <td>0.007526593</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_5</th><br>      <td>-1.411581e+10</td><br>      <td>-0.01231968</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_6</th><br>      <td>1.152954e+10</td><br>      <td>0.005378767</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_7</th><br>      <td>1.308259e+09</td><br>      <td>0.04359522</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_8</th><br>      <td>-5.533222e+09</td><br>      <td>0.02349467</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_9</th><br>      <td>-1.428208e+09</td><br>      <td>0.04563726</td><br>      <td>0.2603355</td><br>    </tr><br>    <tr><br>      <th>var_10</th><br>      <td>-4.916413e+10</td><br>      <td>0.007652379</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_11</th><br>      <td>1.03908e+10</td><br>      <td>-0.01838121</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_12</th><br>      <td>4.468792e+09</td><br>      <td>0.009801968</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_13</th><br>      <td>7.633738e+08</td><br>      <td>-0.03850794</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_14</th><br>      <td>1.796756e+11</td><br>      <td>-0.002881141</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_15</th><br>      <td>-8.060028e+09</td><br>      <td>-0.008913816</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_16</th><br>      <td>1.02906e+09</td><br>      <td>0.02357866</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_17</th><br>      <td>-9.605119e+10</td><br>      <td>-0.001118989</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_18</th><br>      <td>1.551598e+09</td><br>      <td>-0.009704904</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_19</th><br>      <td>-1.837469e+09</td><br>      <td>-0.04449715</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_20</th><br>      <td>-6.597977e+08</td><br>      <td>0.007758192</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_21</th><br>      <td>-2.899382e+09</td><br>      <td>0.03536851</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_22</th><br>      <td>-2.01923</td><br>      <td>-0.005683115</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_23</th><br>      <td>-1.095123</td><br>      <td>0.0124728</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_24</th><br>      <td>-0.4340084</td><br>      <td>-0.005365267</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_25</th><br>      <td>1.247915</td><br>      <td>-0.01624446</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_26</th><br>      <td>-0.2516661</td><br>      <td>-0.005518843</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_27</th><br>      <td>1.718621</td><br>      <td>0.01745864</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>var_28</th><br>      <td>-0.9251402</td><br>      <td>-0.0198678</td><br>      <td>-0</td><br>    </tr><br>    <tr><br>      <th>var_29</th><br>      <td>-0.923786</td><br>      <td>0.003245177</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>R2_IS</th><br>      <td>1</td><br>      <td>0.8156022</td><br>      <td>0.7976887</td><br>    </tr><br>    <tr><br>      <th>R2_OS</th><br>      <td>-82.75313</td><br>      <td>0.6329325</td><br>      <td>0.6522574</td><br>    </tr><br>  </tbody><br></table><br></div>



<h4 id="Problem-1-Cross-validation"><a href="#Problem-1-Cross-validation" class="headerlink" title="Problem 1. Cross-validation."></a>Problem 1. Cross-validation.</h4><p>Certainly on such a small sample performance might largely depend on the split between training, validation and test sets. In order to verify that efficiency of Lasso and Ridge is not just a random consequence of a favorable split perform a cross-validation, reporting average IS/OS performance for each model averaged over 10 different random splits.</p>
<h1 id="Example-4-Predicting-real-estate-prices-using-location-parameters"><a href="#Example-4-Predicting-real-estate-prices-using-location-parameters" class="headerlink" title="Example 4. Predicting real-estate prices using location parameters"></a>Example 4. Predicting real-estate prices using location parameters</h1><p>Before we tried to use some parameters of the house (size) to predict its price. However notice that location matters as well. Below we upload a sample (approximately 30% of the total) of more than 2000 individual house sales all around NYC in 2012. Each record together with the parameters of the house also contains important characteristics of the location (zip code) - average income of its residents (accodring to US census), as well as the relative structure of 311 complains happening in the area.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#load the data</span></span><br><span class="line">data4=pd.read_csv(<span class="string">"data/example4.csv"</span>, low_memory=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data4.head()</span><br></pre></td></tr></table></figure>
<div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>Unnamed: 0</th><br>      <th>Unnamed: 0.1</th><br>      <th>borough</th><br>      <th>neighborhood</th><br>      <th>building_class_category</th><br>      <th>tax_class_present</th><br>      <th>block</th><br>      <th>lot</th><br>      <th>easement</th><br>      <th>building_class_present</th><br>      <th>…</th><br>      <th>Unsanitary Pigeon Condition</th><br>      <th>Urinating in Public</th><br>      <th>Vacant Lot</th><br>      <th>Vending</th><br>      <th>Violation of Park Rules</th><br>      <th>Water Conservation</th><br>      <th>Water Quality</th><br>      <th>Water System</th><br>      <th>Window Guard</th><br>      <th>X Ray Machine Equipment</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>0</td><br>      <td>0</td><br>      <td>4</td><br>      <td>COLLEGE POINT</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>3937</td><br>      <td>31</td><br>      <td>NaN</td><br>      <td>A9</td><br>      <td>…</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>      <td>0.001797</td><br>      <td>0.001540</td><br>      <td>0.003850</td><br>      <td>0.002053</td><br>      <td>0.049281</td><br>      <td>0</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>1</td><br>      <td>1</td><br>      <td>1</td><br>      <td>MIDTOWN EAST</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>1322</td><br>      <td>18</td><br>      <td>NaN</td><br>      <td>A4</td><br>      <td>…</td><br>      <td>0.000227</td><br>      <td>0.000227</td><br>      <td>0.000000</td><br>      <td>0.010675</td><br>      <td>0.000454</td><br>      <td>0.001817</td><br>      <td>0.000227</td><br>      <td>0.050874</td><br>      <td>0</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>2</td><br>      <td>2</td><br>      <td>4</td><br>      <td>SPRINGFIELD GARDENS</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>13029</td><br>      <td>24</td><br>      <td>NaN</td><br>      <td>A1</td><br>      <td>…</td><br>      <td>0.000000</td><br>      <td>0.000000</td><br>      <td>0.004484</td><br>      <td>0.000000</td><br>      <td>0.000179</td><br>      <td>0.007354</td><br>      <td>0.001256</td><br>      <td>0.069058</td><br>      <td>0</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>3</td><br>      <td>3</td><br>      <td>3</td><br>      <td>CROWN HEIGHTS</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>1222</td><br>      <td>53</td><br>      <td>NaN</td><br>      <td>A4</td><br>      <td>…</td><br>      <td>0.000170</td><br>      <td>0.000170</td><br>      <td>0.000908</td><br>      <td>0.000568</td><br>      <td>0.000170</td><br>      <td>0.000454</td><br>      <td>0.000341</td><br>      <td>0.012433</td><br>      <td>0</td><br>      <td>0</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>4</td><br>      <td>4</td><br>      <td>3</td><br>      <td>CYPRESS HILLS</td><br>      <td>01  ONE FAMILY HOMES</td><br>      <td>1</td><br>      <td>3919</td><br>      <td>4</td><br>      <td>NaN</td><br>      <td>A5</td><br>      <td>…</td><br>      <td>0.000491</td><br>      <td>0.000197</td><br>      <td>0.005897</td><br>      <td>0.000590</td><br>      <td>0.000393</td><br>      <td>0.001622</td><br>      <td>0.000442</td><br>      <td>0.037445</td><br>      <td>0</td><br>      <td>0</td><br>    </tr><br>  </tbody><br></table><br><p>5 rows × 184 columns</p><br></div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run OLS</span></span><br><span class="line">y=data4[<span class="string">'sale_price'</span>]</span><br><span class="line">X=data4[<span class="string">'gross_sq_feet'</span>]</span><br><span class="line">result=ols(y=y,x=X)</span><br><span class="line">print(<span class="string">" The R-Squared of this model is:"</span>)</span><br><span class="line">print(result.r2)</span><br><span class="line">result.summary_as_matrix.T</span><br></pre></td></tr></table></figure>
<pre><code> The R-Squared of this model is:
0.329693239244
</code></pre><div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>beta</th><br>      <th>p-value</th><br>      <th>std err</th><br>      <th>t-stat</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>x</th><br>      <td>900.468193</td><br>      <td>2.130323e-208</td><br>      <td>26.357322</td><br>      <td>34.163872</td><br>    </tr><br>    <tr><br>      <th>intercept</th><br>      <td>-1011268.432499</td><br>      <td>1.885970e-74</td><br>      <td>53457.538558</td><br>      <td>-18.917228</td><br>    </tr><br>  </tbody><br></table><br></div>



<p>Even with the intercept, R2 is not too high - perhaps because of the spatial heterogeniety of the pricing. Let’s see if this could be improved by considering other properties of the location but first move to the log scale as the properties of the location are likely to have multiplicative rather than additive impact on the price.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Take logs of our variabes and repeat the regression</span></span><br><span class="line">data4[<span class="string">'sale_price_log'</span>]=np.log(data4[<span class="string">'sale_price'</span>]).round(decimals=<span class="number">3</span>)</span><br><span class="line">data4[<span class="string">'gross_sq_feet_log'</span>]=np.log(data4[<span class="string">'gross_sq_feet'</span>]).round(decimals=<span class="number">3</span>)</span><br><span class="line">y=data4[<span class="string">'sale_price_log'</span>]</span><br><span class="line">X=data4[<span class="string">'gross_sq_feet_log'</span>]</span><br><span class="line">result=ols(y=y,x=X)</span><br><span class="line">print(<span class="string">" The R-Squared of this model is:"</span>)</span><br><span class="line">print(result.r2)</span><br><span class="line">result.summary_as_matrix.T</span><br></pre></td></tr></table></figure>
<pre><code> The R-Squared of this model is:
0.295837837416
</code></pre><div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>beta</th><br>      <th>p-value</th><br>      <th>std err</th><br>      <th>t-stat</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>x</th><br>      <td>0.992956</td><br>      <td>5.519823e-183</td><br>      <td>0.031448</td><br>      <td>31.574711</td><br>    </tr><br>    <tr><br>      <th>intercept</th><br>      <td>5.622858</td><br>      <td>1.502389e-114</td><br>      <td>0.233807</td><br>      <td>24.049125</td><br>    </tr><br>  </tbody><br></table><br></div>



<p>This regression is actually a power-law non-linear model already. No surprize that the slope coefficient for the log of house size is close to 1 with a very narrow confidence interval - one would expect to have the price of the house to be in the approximately linear proportion to its size </p>
<p>Now try adding a first characteristic of the location - average income (also on the log scale)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Take logs of our variabes and repeat the regression</span></span><br><span class="line">data4[<span class="string">'mean_log'</span>]=np.log(data4[<span class="string">'mean'</span>]).round(decimals=<span class="number">3</span>)</span><br><span class="line">y=data4[<span class="string">'sale_price_log'</span>]</span><br><span class="line">X=data4[[<span class="string">'mean_log'</span>,<span class="string">'gross_sq_feet_log'</span>]]</span><br><span class="line">result=ols(y=y,x=X)</span><br><span class="line">print(<span class="string">" The R-Squared of this model is:"</span>)</span><br><span class="line">print(result.r2)</span><br><span class="line">result.summary_as_matrix.T</span><br></pre></td></tr></table></figure>
<pre><code> The R-Squared of this model is:
0.344496603562
</code></pre><div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>beta</th><br>      <th>p-value</th><br>      <th>std err</th><br>      <th>t-stat</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>mean_log</th><br>      <td>0.505073</td><br>      <td>8.132877e-39</td><br>      <td>0.038063</td><br>      <td>13.269373</td><br>    </tr><br>    <tr><br>      <th>gross_sq_feet_log</th><br>      <td>0.928775</td><br>      <td>5.063411e-170</td><br>      <td>0.030731</td><br>      <td>30.222480</td><br>    </tr><br>    <tr><br>      <th>intercept</th><br>      <td>0.441324</td><br>      <td>3.278924e-01</td><br>      <td>0.450989</td><br>      <td>0.978570</td><br>    </tr><br>  </tbody><br></table><br></div>



<p>The model fit is slightly improved and it seems that the price is proportional to the square root of the location wealth. Although wealth is important it seems it does not tell the whole story as 30% R2 is still not too high. Let’s see if the patterns of people complains tell more relevant infromation about the context of the location.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Track the model performance while we add more 311-related variables (R2 vs the number of variables t)</span></span><br><span class="line">list_comp=data4.columns[data4.columns.get_loc(<span class="string">"Adopt A Basket"</span>):data4.columns.get_loc(<span class="string">"X Ray Machine Equipment"</span>)+<span class="number">1</span>]</span><br><span class="line">list_comp=list(list_comp.values.tolist())</span><br><span class="line"></span><br><span class="line"><span class="comment">#In the sample test</span></span><br><span class="line">Number_variables_L=[]</span><br><span class="line">OLS_R_2_L=[]</span><br><span class="line">depend_variable_L=[<span class="string">'mean_log'</span>]</span><br><span class="line">t=<span class="number">0</span></span><br><span class="line"><span class="comment"># Do it for more dependent variables from 311</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> ([<span class="string">'gross_sq_feet_log'</span>]+[list_comp[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(list_comp))]):</span><br><span class="line">    </span><br><span class="line">    t=t+<span class="number">1</span></span><br><span class="line">    Number_variables_L.append(t)</span><br><span class="line">    depend_variable_L.append(j)</span><br><span class="line">    X=data4[depend_variable_L]</span><br><span class="line">    y=data4[<span class="string">'sale_price_log'</span>]</span><br><span class="line">    result=ols(y=y,x=X)</span><br><span class="line">    OLS_R_2_L.append(result.r2)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#####################################################################################</span></span><br><span class="line"></span><br><span class="line">pylab.title(<span class="string">'In sample R-squared'</span>)</span><br><span class="line">pylab.plot(Number_variables_L,OLS_R_2_L,<span class="string">'r'</span>,label=<span class="string">'R_squared(in the sample)'</span>)</span><br><span class="line">pylab.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">pylab.xlabel(<span class="string">'Number of independent variables'</span>)</span><br><span class="line">pylab.ylabel(<span class="string">'R-squared'</span>)</span><br><span class="line">pylab.show()</span><br></pre></td></tr></table></figure>
<p><img src="NBsession2_files/NBsession2_67_0.png" alt="png"></p>
<p>So looks like the new variables did a perfect job rising R2 to 65%! But is the model generalizeable or do we simply overfit? In order to test that let’s split the data into the separate training and test set and re-run the model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">var=len(list_comp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># p= training sample size/ total sample size</span></span><br><span class="line"></span><br><span class="line">p=<span class="number">0.7</span> <span class="comment"># we use 70% data for training and 30% data for prediction.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#####################################################################################################################################</span></span><br><span class="line"><span class="comment"># Split data by p</span></span><br><span class="line"></span><br><span class="line">X=data4</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_index=random.sample(list(range(len(X))),int(len(X)*p))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#####################################################################################################################################</span></span><br><span class="line"><span class="comment">#For the example just fix one of the splits</span></span><br><span class="line">a=[<span class="number">616</span>, <span class="number">2364</span>, <span class="number">2064</span>, <span class="number">1098</span>, <span class="number">726</span>, <span class="number">1866</span>, <span class="number">1649</span>, <span class="number">18</span>, <span class="number">1927</span>, <span class="number">69</span>, <span class="number">1264</span>, <span class="number">1224</span>, <span class="number">1001</span>, <span class="number">2034</span>, <span class="number">1214</span>, <span class="number">444</span>, <span class="number">1665</span>, <span class="number">53</span>, <span class="number">85</span>, <span class="number">617</span>, <span class="number">744</span>, <span class="number">1707</span>, <span class="number">1399</span>, <span class="number">285</span>, <span class="number">1887</span>, <span class="number">96</span>, <span class="number">1861</span>, <span class="number">1121</span>, <span class="number">2247</span>, <span class="number">857</span>, <span class="number">287</span>, <span class="number">1869</span>, <span class="number">1819</span>, <span class="number">2198</span>, <span class="number">2161</span>, <span class="number">507</span>, <span class="number">309</span>, <span class="number">88</span>, <span class="number">784</span>, <span class="number">1382</span>, <span class="number">1340</span>, <span class="number">1668</span>, <span class="number">1291</span>, <span class="number">48</span>, <span class="number">299</span>, <span class="number">2307</span>, <span class="number">2049</span>, <span class="number">646</span>, <span class="number">719</span>, <span class="number">1524</span>, <span class="number">1115</span>, <span class="number">581</span>, <span class="number">1502</span>, <span class="number">2116</span>, <span class="number">445</span>, <span class="number">100</span>, <span class="number">2234</span>, <span class="number">706</span>, <span class="number">955</span>, <span class="number">434</span>, <span class="number">1614</span>, <span class="number">2368</span>, <span class="number">505</span>, <span class="number">298</span>, <span class="number">2355</span>, <span class="number">2133</span>, <span class="number">185</span>, <span class="number">341</span>, <span class="number">737</span>, <span class="number">1599</span>, <span class="number">624</span>, <span class="number">2264</span>, <span class="number">1182</span>, <span class="number">519</span>, <span class="number">1669</span>, <span class="number">632</span>, <span class="number">2313</span>, <span class="number">1755</span>, <span class="number">461</span>, <span class="number">1536</span>, <span class="number">1728</span>, <span class="number">729</span>, <span class="number">111</span>, <span class="number">728</span>, <span class="number">442</span>, <span class="number">1075</span>, <span class="number">884</span>, <span class="number">1085</span>, <span class="number">1225</span>, <span class="number">2294</span>, <span class="number">297</span>, <span class="number">1375</span>, <span class="number">66</span>, <span class="number">2303</span>, <span class="number">83</span>, <span class="number">433</span>, <span class="number">890</span>, <span class="number">2259</span>, <span class="number">1706</span>, <span class="number">864</span>, <span class="number">2358</span>, <span class="number">1908</span>, <span class="number">970</span>, <span class="number">1421</span>, <span class="number">337</span>, <span class="number">2360</span>, <span class="number">267</span>, <span class="number">295</span>, <span class="number">2104</span>, <span class="number">1829</span>, <span class="number">1298</span>, <span class="number">1547</span>, <span class="number">191</span>, <span class="number">991</span>, <span class="number">1565</span>, <span class="number">585</span>, <span class="number">1074</span>, <span class="number">1270</span>, <span class="number">995</span>, <span class="number">1329</span>, <span class="number">1423</span>, <span class="number">1667</span>, <span class="number">1681</span>, <span class="number">157</span>, <span class="number">252</span>, <span class="number">1207</span>, <span class="number">1549</span>, <span class="number">40</span>, <span class="number">983</span>, <span class="number">177</span>, <span class="number">975</span>, <span class="number">972</span>, <span class="number">684</span>, <span class="number">1337</span>, <span class="number">1231</span>, <span class="number">1124</span>, <span class="number">1655</span>, <span class="number">1682</span>, <span class="number">273</span>, <span class="number">1359</span>, <span class="number">1641</span>, <span class="number">1371</span>, <span class="number">1806</span>, <span class="number">1513</span>, <span class="number">220</span>, <span class="number">1436</span>, <span class="number">1874</span>, <span class="number">555</span>, <span class="number">2192</span>, <span class="number">2216</span>, <span class="number">763</span>, <span class="number">1317</span>, <span class="number">87</span>, <span class="number">1218</span>, <span class="number">1188</span>, <span class="number">1751</span>, <span class="number">324</span>, <span class="number">1489</span>, <span class="number">1301</span>, <span class="number">79</span>, <span class="number">985</span>, <span class="number">2086</span>, <span class="number">1281</span>, <span class="number">1888</span>, <span class="number">179</span>, <span class="number">1997</span>, <span class="number">1174</span>, <span class="number">1515</span>, <span class="number">1252</span>, <span class="number">1240</span>, <span class="number">283</span>, <span class="number">380</span>, <span class="number">2105</span>, <span class="number">1942</span>, <span class="number">1753</span>, <span class="number">428</span>, <span class="number">381</span>, <span class="number">667</span>, <span class="number">190</span>, <span class="number">1817</span>, <span class="number">690</span>, <span class="number">1361</span>, <span class="number">1936</span>, <span class="number">2332</span>, <span class="number">1746</span>, <span class="number">939</span>, <span class="number">2310</span>, <span class="number">1875</span>, <span class="number">543</span>, <span class="number">740</span>, <span class="number">548</span>, <span class="number">1160</span>, <span class="number">1413</span>, <span class="number">2299</span>, <span class="number">1602</span>, <span class="number">2370</span>, <span class="number">2012</span>, <span class="number">392</span>, <span class="number">794</span>, <span class="number">1999</span>, <span class="number">495</span>, <span class="number">1613</span>, <span class="number">2180</span>, <span class="number">478</span>, <span class="number">732</span>, <span class="number">1760</span>, <span class="number">1787</span>, <span class="number">259</span>, <span class="number">936</span>, <span class="number">820</span>, <span class="number">1770</span>, <span class="number">497</span>, <span class="number">2329</span>, <span class="number">2280</span>, <span class="number">964</span>, <span class="number">1537</span>, <span class="number">1801</span>, <span class="number">1247</span>, <span class="number">1656</span>, <span class="number">685</span>, <span class="number">1930</span>, <span class="number">2031</span>, <span class="number">1661</span>, <span class="number">1675</span>, <span class="number">1553</span>, <span class="number">151</span>, <span class="number">2203</span>, <span class="number">2125</span>, <span class="number">2162</span>, <span class="number">1130</span>, <span class="number">1860</span>, <span class="number">1500</span>, <span class="number">2124</span>, <span class="number">2320</span>, <span class="number">1122</span>, <span class="number">2160</span>, <span class="number">1117</span>, <span class="number">800</span>, <span class="number">1447</span>, <span class="number">1945</span>, <span class="number">121</span>, <span class="number">1870</span>, <span class="number">2330</span>, <span class="number">1487</span>, <span class="number">188</span>, <span class="number">859</span>, <span class="number">1333</span>, <span class="number">1420</span>, <span class="number">1315</span>, <span class="number">1815</span>, <span class="number">1216</span>, <span class="number">1802</span>, <span class="number">272</span>, <span class="number">1961</span>, <span class="number">1415</span>, <span class="number">547</span>, <span class="number">2150</span>, <span class="number">1617</span>, <span class="number">2221</span>, <span class="number">2070</span>, <span class="number">1400</span>, <span class="number">789</span>, <span class="number">1653</span>, <span class="number">1494</span>, <span class="number">889</span>, <span class="number">1876</span>, <span class="number">1467</span>, <span class="number">1254</span>, <span class="number">1147</span>, <span class="number">1848</span>, <span class="number">775</span>, <span class="number">1127</span>, <span class="number">1699</span>, <span class="number">2213</span>, <span class="number">2246</span>, <span class="number">1858</span>, <span class="number">1398</span>, <span class="number">477</span>, <span class="number">1544</span>, <span class="number">42</span>, <span class="number">1282</span>, <span class="number">496</span>, <span class="number">1872</span>, <span class="number">168</span>, <span class="number">2107</span>, <span class="number">300</span>, <span class="number">1307</span>, <span class="number">1523</span>, <span class="number">1027</span>, <span class="number">946</span>, <span class="number">1955</span>, <span class="number">1468</span>, <span class="number">541</span>, <span class="number">1652</span>, <span class="number">382</span>, <span class="number">2036</span>, <span class="number">662</span>, <span class="number">827</span>, <span class="number">1984</span>, <span class="number">780</span>, <span class="number">161</span>, <span class="number">1776</span>, <span class="number">638</span>, <span class="number">611</span>, <span class="number">1525</span>, <span class="number">793</span>, <span class="number">2139</span>, <span class="number">1140</span>, <span class="number">1320</span>, <span class="number">1455</span>, <span class="number">1953</span>, <span class="number">2151</span>, <span class="number">86</span>, <span class="number">1634</span>, <span class="number">1671</span>, <span class="number">743</span>, <span class="number">464</span>, <span class="number">856</span>, <span class="number">1163</span>, <span class="number">399</span>, <span class="number">395</span>, <span class="number">413</span>, <span class="number">2015</span>, <span class="number">313</span>, <span class="number">90</span>, <span class="number">1062</span>, <span class="number">2148</span>, <span class="number">388</span>, <span class="number">1280</span>, <span class="number">1685</span>, <span class="number">2334</span>, <span class="number">2369</span>, <span class="number">1491</span>, <span class="number">1080</span>, <span class="number">348</span>, <span class="number">928</span>, <span class="number">1412</span>, <span class="number">500</span>, <span class="number">1079</span>, <span class="number">181</span>, <span class="number">117</span>, <span class="number">1103</span>, <span class="number">761</span>, <span class="number">2352</span>, <span class="number">2009</span>, <span class="number">1227</span>, <span class="number">1131</span>, <span class="number">2188</span>, <span class="number">1272</span>, <span class="number">449</span>, <span class="number">672</span>, <span class="number">1049</span>, <span class="number">516</span>, <span class="number">2095</span>, <span class="number">2229</span>, <span class="number">1604</span>, <span class="number">1434</span>, <span class="number">102</span>, <span class="number">826</span>, <span class="number">212</span>, <span class="number">430</span>, <span class="number">1128</span>, <span class="number">318</span>, <span class="number">2082</span>, <span class="number">1007</span>, <span class="number">2025</span>, <span class="number">1825</span>, <span class="number">296</span>, <span class="number">676</span>, <span class="number">2220</span>, <span class="number">1938</span>, <span class="number">94</span>, <span class="number">373</span>, <span class="number">1139</span>, <span class="number">1198</span>, <span class="number">1883</span>, <span class="number">1910</span>, <span class="number">1636</span>, <span class="number">935</span>, <span class="number">2163</span>, <span class="number">465</span>, <span class="number">1003</span>, <span class="number">2367</span>, <span class="number">405</span>, <span class="number">896</span>, <span class="number">205</span>, <span class="number">2242</span>, <span class="number">1081</span>, <span class="number">1792</span>, <span class="number">2142</span>, <span class="number">1847</span>, <span class="number">1292</span>, <span class="number">415</span>, <span class="number">1120</span>, <span class="number">1892</span>, <span class="number">661</span>, <span class="number">1099</span>, <span class="number">1909</span>, <span class="number">1054</span>, <span class="number">1623</span>, <span class="number">1208</span>, <span class="number">2018</span>, <span class="number">2020</span>, <span class="number">1167</span>, <span class="number">1917</span>, <span class="number">391</span>, <span class="number">1985</span>, <span class="number">331</span>, <span class="number">749</span>, <span class="number">379</span>, <span class="number">796</span>, <span class="number">1732</span>, <span class="number">879</span>, <span class="number">881</span>, <span class="number">1522</span>, <span class="number">1265</span>, <span class="number">422</span>, <span class="number">1406</span>, <span class="number">319</span>, <span class="number">81</span>, <span class="number">1974</span>, <span class="number">223</span>, <span class="number">226</span>, <span class="number">1968</span>, <span class="number">1711</span>, <span class="number">2182</span>, <span class="number">467</span>, <span class="number">1228</span>, <span class="number">1601</span>, <span class="number">974</span>, <span class="number">1810</span>, <span class="number">1429</span>, <span class="number">1657</span>, <span class="number">71</span>, <span class="number">1768</span>, <span class="number">734</span>, <span class="number">1284</span>, <span class="number">1411</span>, <span class="number">339</span>, <span class="number">1342</span>, <span class="number">573</span>, <span class="number">2208</span>, <span class="number">1804</span>, <span class="number">2113</span>, <span class="number">814</span>, <span class="number">261</span>, <span class="number">1066</span>, <span class="number">851</span>, <span class="number">1737</span>, <span class="number">2173</span>, <span class="number">920</span>, <span class="number">276</span>, <span class="number">659</span>, <span class="number">862</span>, <span class="number">84</span>, <span class="number">2248</span>, <span class="number">1519</span>, <span class="number">839</span>, <span class="number">1</span>, <span class="number">1716</span>, <span class="number">776</span>, <span class="number">1465</span>, <span class="number">1477</span>, <span class="number">1508</span>, <span class="number">664</span>, <span class="number">1626</span>, <span class="number">506</span>, <span class="number">556</span>, <span class="number">1492</span>, <span class="number">2021</span>, <span class="number">1237</span>, <span class="number">125</span>, <span class="number">1625</span>, <span class="number">193</span>, <span class="number">215</span>, <span class="number">182</span>, <span class="number">60</span>, <span class="number">2201</span>, <span class="number">2051</span>, <span class="number">630</span>, <span class="number">2103</span>, <span class="number">786</span>, <span class="number">2271</span>, <span class="number">765</span>, <span class="number">944</span>, <span class="number">375</span>, <span class="number">911</span>, <span class="number">960</span>, <span class="number">1046</span>, <span class="number">2235</span>, <span class="number">1919</span>, <span class="number">340</span>, <span class="number">1246</span>, <span class="number">932</span>, <span class="number">2014</span>, <span class="number">229</span>, <span class="number">2100</span>, <span class="number">311</span>, <span class="number">1067</span>, <span class="number">2187</span>, <span class="number">986</span>, <span class="number">425</span>, <span class="number">610</span>, <span class="number">1637</span>, <span class="number">365</span>, <span class="number">251</span>, <span class="number">1748</span>, <span class="number">472</span>, <span class="number">992</span>, <span class="number">256</span>, <span class="number">822</span>, <span class="number">502</span>, <span class="number">824</span>, <span class="number">67</span>, <span class="number">1507</span>, <span class="number">1590</span>, <span class="number">1725</span>, <span class="number">1034</span>, <span class="number">1995</span>, <span class="number">651</span>, <span class="number">2273</span>, <span class="number">643</span>, <span class="number">1493</span>, <span class="number">1470</span>, <span class="number">370</span>, <span class="number">637</span>, <span class="number">779</span>, <span class="number">1924</span>, <span class="number">2285</span>, <span class="number">2065</span>, <span class="number">933</span>, <span class="number">2096</span>, <span class="number">1934</span>, <span class="number">2288</span>, <span class="number">2033</span>, <span class="number">2184</span>, <span class="number">948</span>, <span class="number">70</span>, <span class="number">95</span>, <span class="number">58</span>, <span class="number">2304</span>, <span class="number">1624</span>, <span class="number">2236</span>, <span class="number">1426</span>, <span class="number">266</span>, <span class="number">2109</span>, <span class="number">612</span>, <span class="number">1136</span>, <span class="number">1606</span>, <span class="number">709</span>, <span class="number">590</span>, <span class="number">823</span>, <span class="number">2146</span>, <span class="number">1578</span>, <span class="number">1956</span>, <span class="number">1616</span>, <span class="number">1569</span>, <span class="number">25</span>, <span class="number">756</span>, <span class="number">751</span>, <span class="number">1798</span>, <span class="number">818</span>, <span class="number">1586</span>, <span class="number">284</span>, <span class="number">351</span>, <span class="number">1784</span>, <span class="number">806</span>, <span class="number">1698</span>, <span class="number">2090</span>, <span class="number">1232</span>, <span class="number">1379</span>, <span class="number">288</span>, <span class="number">1192</span>, <span class="number">2054</span>, <span class="number">1381</span>, <span class="number">1922</span>, <span class="number">2069</span>, <span class="number">93</span>, <span class="number">1680</span>, <span class="number">376</span>, <span class="number">1369</span>, <span class="number">2152</span>, <span class="number">1233</span>, <span class="number">1155</span>, <span class="number">486</span>, <span class="number">1212</span>, <span class="number">362</span>, <span class="number">1509</span>, <span class="number">1539</span>, <span class="number">2275</span>, <span class="number">1505</span>, <span class="number">1777</span>, <span class="number">44</span>, <span class="number">1990</span>, <span class="number">1980</span>, <span class="number">343</span>, <span class="number">1797</span>, <span class="number">1731</span>, <span class="number">679</span>, <span class="number">322</span>, <span class="number">1773</span>, <span class="number">1324</span>, <span class="number">792</span>, <span class="number">666</span>, <span class="number">2177</span>, <span class="number">2145</span>, <span class="number">1168</span>, <span class="number">148</span>, <span class="number">80</span>, <span class="number">627</span>, <span class="number">498</span>, <span class="number">1552</span>, <span class="number">280</span>, <span class="number">1437</span>, <span class="number">644</span>, <span class="number">602</span>, <span class="number">1952</span>, <span class="number">521</span>, <span class="number">140</span>, <span class="number">2004</span>, <span class="number">183</span>, <span class="number">320</span>, <span class="number">1253</span>, <span class="number">357</span>, <span class="number">1794</span>, <span class="number">943</span>, <span class="number">1622</span>, <span class="number">1643</span>, <span class="number">1905</span>, <span class="number">1071</span>, <span class="number">2067</span>, <span class="number">2119</span>, <span class="number">1821</span>, <span class="number">746</span>, <span class="number">1295</span>, <span class="number">886</span>, <span class="number">1842</span>, <span class="number">2174</span>, <span class="number">1450</span>, <span class="number">2200</span>, <span class="number">2005</span>, <span class="number">957</span>, <span class="number">162</span>, <span class="number">1015</span>, <span class="number">1558</span>, <span class="number">1205</span>, <span class="number">2011</span>, <span class="number">353</span>, <span class="number">305</span>, <span class="number">769</span>, <span class="number">1738</span>, <span class="number">1309</span>, <span class="number">623</span>, <span class="number">2140</span>, <span class="number">1764</span>, <span class="number">981</span>, <span class="number">126</span>, <span class="number">1365</span>, <span class="number">1262</span>, <span class="number">1289</span>, <span class="number">1763</span>, <span class="number">927</span>, <span class="number">201</span>, <span class="number">1618</span>, <span class="number">635</span>, <span class="number">2042</span>, <span class="number">1377</span>, <span class="number">1989</span>, <span class="number">2361</span>, <span class="number">130</span>, <span class="number">1805</span>, <span class="number">535</span>, <span class="number">598</span>, <span class="number">1360</span>, <span class="number">1057</span>, <span class="number">1608</span>, <span class="number">990</span>, <span class="number">907</span>, <span class="number">2144</span>, <span class="number">209</span>, <span class="number">591</span>, <span class="number">312</span>, <span class="number">78</span>, <span class="number">327</span>, <span class="number">1551</span>, <span class="number">1169</span>, <span class="number">1664</span>, <span class="number">1563</span>, <span class="number">55</span>, <span class="number">1918</span>, <span class="number">14</span>, <span class="number">565</span>, <span class="number">1030</span>, <span class="number">673</span>, <span class="number">563</span>, <span class="number">1611</span>, <span class="number">426</span>, <span class="number">1631</span>, <span class="number">1220</span>, <span class="number">575</span>, <span class="number">1417</span>, <span class="number">2157</span>, <span class="number">1101</span>, <span class="number">1407</span>, <span class="number">2309</span>, <span class="number">1016</span>, <span class="number">969</span>, <span class="number">1937</span>, <span class="number">2322</span>, <span class="number">1261</span>, <span class="number">237</span>, <span class="number">338</span>, <span class="number">1988</span>, <span class="number">1780</span>, <span class="number">2336</span>, <span class="number">1323</span>, <span class="number">1314</span>, <span class="number">403</span>, <span class="number">870</span>, <span class="number">314</span>, <span class="number">526</span>, <span class="number">2063</span>, <span class="number">1458</span>, <span class="number">1195</span>, <span class="number">367</span>, <span class="number">1851</span>, <span class="number">361</span>, <span class="number">2227</span>, <span class="number">371</span>, <span class="number">1568</span>, <span class="number">1572</span>, <span class="number">1137</span>, <span class="number">1654</span>, <span class="number">439</span>, <span class="number">1564</span>, <span class="number">345</span>, <span class="number">588</span>, <span class="number">2232</span>, <span class="number">2183</span>, <span class="number">514</span>, <span class="number">938</span>, <span class="number">1061</span>, <span class="number">141</span>, <span class="number">160</span>, <span class="number">1704</span>, <span class="number">2204</span>, <span class="number">1931</span>, <span class="number">1644</span>, <span class="number">1761</span>, <span class="number">511</span>, <span class="number">423</span>, <span class="number">115</span>, <span class="number">211</span>, <span class="number">924</span>, <span class="number">1389</span>, <span class="number">1683</span>, <span class="number">1257</span>, <span class="number">1238</span>, <span class="number">291</span>, <span class="number">1589</span>, <span class="number">1148</span>, <span class="number">1735</span>, <span class="number">2345</span>, <span class="number">1827</span>, <span class="number">993</span>, <span class="number">783</span>, <span class="number">198</span>, <span class="number">1852</span>, <span class="number">1215</span>, <span class="number">1069</span>, <span class="number">1841</span>, <span class="number">1499</span>, <span class="number">1296</span>, <span class="number">1901</span>, <span class="number">7</span>, <span class="number">173</span>, <span class="number">925</span>, <span class="number">1868</span>, <span class="number">1402</span>, <span class="number">2347</span>, <span class="number">1091</span>, <span class="number">1351</span>, <span class="number">239</span>, <span class="number">186</span>, <span class="number">2351</span>, <span class="number">544</span>, <span class="number">2176</span>, <span class="number">1718</span>, <span class="number">2193</span>, <span class="number">1395</span>, <span class="number">315</span>, <span class="number">596</span>, <span class="number">1367</span>, <span class="number">1528</span>, <span class="number">1339</span>, <span class="number">594</span>, <span class="number">1650</span>, <span class="number">605</span>, <span class="number">1459</span>, <span class="number">1793</span>, <span class="number">1812</span>, <span class="number">1935</span>, <span class="number">1891</span>, <span class="number">2306</span>, <span class="number">1674</span>, <span class="number">1325</span>, <span class="number">710</span>, <span class="number">2072</span>, <span class="number">1322</span>, <span class="number">2308</span>, <span class="number">1702</span>, <span class="number">289</span>, <span class="number">968</span>, <span class="number">1312</span>, <span class="number">1774</span>, <span class="number">2269</span>, <span class="number">725</span>, <span class="number">1445</span>, <span class="number">2073</span>, <span class="number">1579</span>, <span class="number">1534</span>, <span class="number">1963</span>, <span class="number">1973</span>, <span class="number">171</span>, <span class="number">1345</span>, <span class="number">2270</span>, <span class="number">97</span>, <span class="number">1592</span>, <span class="number">199</span>, <span class="number">1236</span>, <span class="number">230</span>, <span class="number">1496</span>, <span class="number">680</span>, <span class="number">2262</span>, <span class="number">1111</span>, <span class="number">967</span>, <span class="number">2057</span>, <span class="number">1279</span>, <span class="number">2277</span>, <span class="number">508</span>, <span class="number">1454</span>, <span class="number">904</span>, <span class="number">1779</span>, <span class="number">34</span>, <span class="number">694</span>, <span class="number">1574</span>, <span class="number">1348</span>, <span class="number">2219</span>, <span class="number">1442</span>, <span class="number">192</span>, <span class="number">2048</span>, <span class="number">858</span>, <span class="number">2350</span>, <span class="number">265</span>, <span class="number">59</span>, <span class="number">852</span>, <span class="number">1837</span>, <span class="number">2108</span>, <span class="number">1757</span>, <span class="number">1660</span>, <span class="number">1554</span>, <span class="number">2071</span>, <span class="number">1355</span>, <span class="number">642</span>, <span class="number">2038</span>, <span class="number">1330</span>, <span class="number">705</span>, <span class="number">1697</span>, <span class="number">1778</span>, <span class="number">1879</span>, <span class="number">649</span>, <span class="number">1639</span>, <span class="number">2314</span>, <span class="number">817</span>, <span class="number">771</span>, <span class="number">768</span>, <span class="number">1178</span>, <span class="number">203</span>, <span class="number">2022</span>, <span class="number">124</span>, <span class="number">1818</span>, <span class="number">554</span>, <span class="number">166</span>, <span class="number">669</span>, <span class="number">1230</span>, <span class="number">1724</span>, <span class="number">953</span>, <span class="number">2316</span>, <span class="number">1977</span>, <span class="number">1789</span>, <span class="number">965</span>, <span class="number">258</span>, <span class="number">801</span>, <span class="number">754</span>, <span class="number">2030</span>, <span class="number">47</span>, <span class="number">1191</span>, <span class="number">404</span>, <span class="number">2365</span>, <span class="number">875</span>, <span class="number">1520</span>, <span class="number">2156</span>, <span class="number">530</span>, <span class="number">1488</span>, <span class="number">882</span>, <span class="number">387</span>, <span class="number">532</span>, <span class="number">1567</span>, <span class="number">136</span>, <span class="number">1078</span>, <span class="number">1439</span>, <span class="number">1204</span>, <span class="number">1146</span>, <span class="number">586</span>, <span class="number">33</span>, <span class="number">562</span>, <span class="number">354</span>, <span class="number">522</span>, <span class="number">727</span>, <span class="number">1734</span>, <span class="number">762</span>, <span class="number">1913</span>, <span class="number">1308</span>, <span class="number">24</span>, <span class="number">892</span>, <span class="number">488</span>, <span class="number">269</span>, <span class="number">56</span>, <span class="number">1878</span>, <span class="number">1479</span>, <span class="number">175</span>, <span class="number">1024</span>, <span class="number">1692</span>, <span class="number">626</span>, <span class="number">1824</span>, <span class="number">1332</span>, <span class="number">1475</span>, <span class="number">1328</span>, <span class="number">2083</span>, <span class="number">1729</span>, <span class="number">700</span>, <span class="number">815</span>, <span class="number">517</span>, <span class="number">180</span>, <span class="number">492</span>, <span class="number">1144</span>, <span class="number">213</span>, <span class="number">696</span>, <span class="number">922</span>, <span class="number">941</span>, <span class="number">17</span>, <span class="number">523</span>, <span class="number">621</span>, <span class="number">2121</span>, <span class="number">663</span>, <span class="number">2357</span>, <span class="number">2293</span>, <span class="number">417</span>, <span class="number">23</span>, <span class="number">243</span>, <span class="number">1021</span>, <span class="number">2267</span>, <span class="number">217</span>, <span class="number">154</span>, <span class="number">2356</span>, <span class="number">2325</span>, <span class="number">26</span>, <span class="number">1302</span>, <span class="number">1185</span>, <span class="number">2023</span>, <span class="number">326</span>, <span class="number">1032</span>, <span class="number">424</span>, <span class="number">2274</span>, <span class="number">1052</span>, <span class="number">1820</span>, <span class="number">2279</span>, <span class="number">2359</span>, <span class="number">2199</span>, <span class="number">2158</span>, <span class="number">2348</span>, <span class="number">2196</span>, <span class="number">1409</span>, <span class="number">2055</span>, <span class="number">369</span>, <span class="number">1304</span>, <span class="number">1864</span>, <span class="number">2029</span>, <span class="number">494</span>, <span class="number">1979</span>, <span class="number">268</span>, <span class="number">1826</span>, <span class="number">12</span>, <span class="number">2167</span>, <span class="number">2237</span>, <span class="number">1347</span>, <span class="number">235</span>, <span class="number">1431</span>, <span class="number">574</span>, <span class="number">1865</span>, <span class="number">567</span>, <span class="number">809</span>, <span class="number">290</span>, <span class="number">214</span>, <span class="number">457</span>, <span class="number">1433</span>, <span class="number">1064</span>, <span class="number">254</span>, <span class="number">1453</span>, <span class="number">402</span>, <span class="number">2212</span>, <span class="number">1752</span>, <span class="number">463</span>, <span class="number">741</span>, <span class="number">43</span>, <span class="number">738</span>, <span class="number">1176</span>, <span class="number">2027</span>, <span class="number">2249</span>, <span class="number">1605</span>, <span class="number">1809</span>, <span class="number">1326</span>, <span class="number">966</span>, <span class="number">108</span>, <span class="number">187</span>, <span class="number">2078</span>, <span class="number">1338</span>, <span class="number">717</span>, <span class="number">144</span>, <span class="number">1882</span>, <span class="number">1939</span>, <span class="number">120</span>, <span class="number">1435</span>, <span class="number">62</span>, <span class="number">2250</span>, <span class="number">1106</span>, <span class="number">914</span>, <span class="number">436</span>, <span class="number">38</span>, <span class="number">2075</span>, <span class="number">2300</span>, <span class="number">360</span>, <span class="number">2265</span>, <span class="number">600</span>, <span class="number">843</span>, <span class="number">831</span>, <span class="number">2190</span>, <span class="number">866</span>, <span class="number">1278</span>, <span class="number">1019</span>, <span class="number">1849</span>, <span class="number">2040</span>, <span class="number">480</span>, <span class="number">1456</span>, <span class="number">1972</span>, <span class="number">400</span>, <span class="number">2058</span>, <span class="number">670</span>, <span class="number">1408</span>, <span class="number">19</span>, <span class="number">2132</span>, <span class="number">37</span>, <span class="number">1267</span>, <span class="number">2231</span>, <span class="number">1388</span>, <span class="number">641</span>, <span class="number">1229</span>, <span class="number">958</span>, <span class="number">227</span>, <span class="number">959</span>, <span class="number">482</span>, <span class="number">1966</span>, <span class="number">926</span>, <span class="number">2</span>, <span class="number">2339</span>, <span class="number">210</span>, <span class="number">613</span>, <span class="number">1762</span>, <span class="number">349</span>, <span class="number">1473</span>, <span class="number">515</span>, <span class="number">2205</span>, <span class="number">165</span>, <span class="number">552</span>, <span class="number">1384</span>, <span class="number">1533</span>, <span class="number">1033</span>, <span class="number">1571</span>, <span class="number">570</span>, <span class="number">2289</span>, <span class="number">529</span>, <span class="number">189</span>, <span class="number">1719</span>, <span class="number">2019</span>, <span class="number">675</span>, <span class="number">1383</span>, <span class="number">1495</span>, <span class="number">233</span>, <span class="number">1463</span>, <span class="number">21</span>, <span class="number">1142</span>, <span class="number">2331</span>, <span class="number">1401</span>, <span class="number">328</span>, <span class="number">1331</span>, <span class="number">419</span>, <span class="number">308</span>, <span class="number">634</span>, <span class="number">1744</span>, <span class="number">325</span>, <span class="number">853</span>, <span class="number">533</span>, <span class="number">248</span>, <span class="number">1987</span>, <span class="number">1679</span>, <span class="number">945</span>, <span class="number">279</span>, <span class="number">639</span>, <span class="number">1800</span>, <span class="number">1890</span>, <span class="number">524</span>, <span class="number">545</span>, <span class="number">1440</span>, <span class="number">1343</span>, <span class="number">1275</span>, <span class="number">2282</span>, <span class="number">1076</span>, <span class="number">1094</span>, <span class="number">2298</span>, <span class="number">1978</span>, <span class="number">1143</span>, <span class="number">869</span>, <span class="number">698</span>, <span class="number">307</span>, <span class="number">1154</span>, <span class="number">1486</span>, <span class="number">1020</span>, <span class="number">1584</span>, <span class="number">905</span>, <span class="number">2143</span>, <span class="number">897</span>, <span class="number">1678</span>, <span class="number">2362</span>, <span class="number">2010</span>, <span class="number">1045</span>, <span class="number">386</span>, <span class="number">1814</span>, <span class="number">833</span>, <span class="number">1766</span>, <span class="number">432</span>, <span class="number">1416</span>, <span class="number">1898</span>, <span class="number">1100</span>, <span class="number">1630</span>, <span class="number">1072</span>, <span class="number">323</span>, <span class="number">1153</span>, <span class="number">891</span>, <span class="number">609</span>, <span class="number">1585</span>, <span class="number">665</span>, <span class="number">355</span>, <span class="number">1722</span>, <span class="number">2175</span>, <span class="number">147</span>, <span class="number">2214</span>, <span class="number">877</span>, <span class="number">2255</span>, <span class="number">781</span>, <span class="number">1422</span>, <span class="number">390</span>, <span class="number">1438</span>, <span class="number">1928</span>, <span class="number">686</span>, <span class="number">1095</span>, <span class="number">1242</span>, <span class="number">1480</span>, <span class="number">1853</span>, <span class="number">350</span>, <span class="number">668</span>, <span class="number">454</span>, <span class="number">546</span>, <span class="number">1562</span>, <span class="number">1083</span>, <span class="number">219</span>, <span class="number">636</span>, <span class="number">1511</span>, <span class="number">1673</span>, <span class="number">1180</span>, <span class="number">169</span>, <span class="number">722</span>, <span class="number">1427</span>, <span class="number">127</span>, <span class="number">329</span>, <span class="number">489</span>, <span class="number">810</span>, <span class="number">207</span>, <span class="number">206</span>, <span class="number">2059</span>, <span class="number">982</span>, <span class="number">802</span>, <span class="number">1709</span>, <span class="number">1035</span>, <span class="number">1060</span>, <span class="number">394</span>, <span class="number">1217</span>, <span class="number">2172</span>, <span class="number">470</span>, <span class="number">2302</span>, <span class="number">902</span>, <span class="number">99</span>, <span class="number">640</span>, <span class="number">2093</span>, <span class="number">1336</span>, <span class="number">358</span>, <span class="number">366</span>, <span class="number">1352</span>, <span class="number">2189</span>, <span class="number">1600</span>, <span class="number">2253</span>, <span class="number">156</span>, <span class="number">693</span>, <span class="number">597</span>, <span class="number">1134</span>, <span class="number">2263</span>, <span class="number">980</span>, <span class="number">1896</span>, <span class="number">1529</span>, <span class="number">2043</span>, <span class="number">2159</span>, <span class="number">255</span>, <span class="number">1929</span>, <span class="number">1104</span>, <span class="number">1832</span>, <span class="number">1464</span>, <span class="number">1873</span>, <span class="number">2002</span>, <span class="number">1047</span>, <span class="number">1368</span>, <span class="number">607</span>, <span class="number">1397</span>, <span class="number">2186</span>, <span class="number">528</span>, <span class="number">200</span>, <span class="number">476</span>, <span class="number">1023</span>, <span class="number">401</span>, <span class="number">1686</span>, <span class="number">202</span>, <span class="number">1867</span>, <span class="number">1967</span>, <span class="number">484</span>, <span class="number">1138</span>, <span class="number">1460</span>, <span class="number">854</span>, <span class="number">1028</span>, <span class="number">619</span>, <span class="number">961</span>, <span class="number">1958</span>, <span class="number">77</span>, <span class="number">50</span>, <span class="number">2290</span>, <span class="number">2281</span>, <span class="number">1715</span>, <span class="number">344</span>, <span class="number">901</span>, <span class="number">629</span>, <span class="number">1976</span>, <span class="number">1526</span>, <span class="number">1521</span>, <span class="number">2165</span>, <span class="number">347</span>, <span class="number">332</span>, <span class="number">865</span>, <span class="number">1689</span>, <span class="number">1951</span>, <span class="number">1376</span>, <span class="number">2080</span>, <span class="number">1044</span>, <span class="number">1886</span>, <span class="number">2007</span>, <span class="number">797</span>, <span class="number">1739</span>, <span class="number">671</span>, <span class="number">1363</span>, <span class="number">1555</span>, <span class="number">1183</span>, <span class="number">409</span>, <span class="number">799</span>, <span class="number">1628</span>, <span class="number">531</span>, <span class="number">1089</span>, <span class="number">942</span>, <span class="number">1449</span>, <span class="number">1335</span>, <span class="number">795</span>, <span class="number">622</span>, <span class="number">2323</span>, <span class="number">164</span>, <span class="number">1897</span>, <span class="number">238</span>, <span class="number">984</span>, <span class="number">1831</span>, <span class="number">861</span>, <span class="number">2123</span>, <span class="number">204</span>, <span class="number">509</span>, <span class="number">1184</span>, <span class="number">167</span>, <span class="number">2138</span>, <span class="number">1073</span>, <span class="number">2268</span>, <span class="number">82</span>, <span class="number">872</span>, <span class="number">1025</span>, <span class="number">234</span>, <span class="number">1531</span>, <span class="number">829</span>, <span class="number">1485</span>, <span class="number">2130</span>, <span class="number">2226</span>, <span class="number">1700</span>, <span class="number">2062</span>, <span class="number">208</span>, <span class="number">1065</span>, <span class="number">1740</span>, <span class="number">1132</span>, <span class="number">954</span>, <span class="number">112</span>, <span class="number">702</span>, <span class="number">1165</span>, <span class="number">1803</span>, <span class="number">1807</span>, <span class="number">1223</span>, <span class="number">952</span>, <span class="number">2118</span>, <span class="number">592</span>, <span class="number">579</span>, <span class="number">1741</span>, <span class="number">1425</span>, <span class="number">2111</span>, <span class="number">253</span>, <span class="number">1211</span>, <span class="number">1392</span>, <span class="number">918</span>, <span class="number">2238</span>, <span class="number">2366</span>, <span class="number">1201</span>, <span class="number">910</span>, <span class="number">1844</span>, <span class="number">1171</span>, <span class="number">1850</span>, <span class="number">2245</span>, <span class="number">2258</span>, <span class="number">195</span>, <span class="number">504</span>, <span class="number">1039</span>, <span class="number">1251</span>, <span class="number">150</span>, <span class="number">1444</span>, <span class="number">1705</span>, <span class="number">359</span>, <span class="number">2041</span>, <span class="number">1471</span>, <span class="number">739</span>, <span class="number">647</span>, <span class="number">119</span>, <span class="number">224</span>, <span class="number">1540</span>, <span class="number">2287</span>, <span class="number">1287</span>, <span class="number">146</span>, <span class="number">1941</span>, <span class="number">6</span>, <span class="number">840</span>, <span class="number">2099</span>, <span class="number">1288</span>, <span class="number">1998</span>, <span class="number">2087</span>, <span class="number">2251</span>, <span class="number">2254</span>, <span class="number">2050</span>, <span class="number">2006</span>, <span class="number">540</span>, <span class="number">1014</span>, <span class="number">1114</span>, <span class="number">977</span>, <span class="number">1932</span>, <span class="number">733</span>, <span class="number">2321</span>, <span class="number">336</span>, <span class="number">1687</span>, <span class="number">1018</span>, <span class="number">1629</span>, <span class="number">1159</span>, <span class="number">466</span>, <span class="number">1164</span>, <span class="number">1206</span>, <span class="number">1259</span>, <span class="number">1394</span>, <span class="number">1290</span>, <span class="number">1474</span>, <span class="number">1266</span>, <span class="number">1912</span>, <span class="number">819</span>, <span class="number">2074</span>, <span class="number">1151</span>, <span class="number">745</span>, <span class="number">1226</span>, <span class="number">1187</span>, <span class="number">2106</span>, <span class="number">1855</span>, <span class="number">1173</span>, <span class="number">876</span>, <span class="number">2363</span>, <span class="number">91</span>, <span class="number">1469</span>, <span class="number">1037</span>, <span class="number">2147</span>, <span class="number">1621</span>, <span class="number">2278</span>, <span class="number">2374</span>, <span class="number">1498</span>, <span class="number">1077</span>, <span class="number">29</span>, <span class="number">973</span>, <span class="number">1109</span>, <span class="number">1696</span>, <span class="number">1084</span>, <span class="number">841</span>, <span class="number">2286</span>, <span class="number">363</span>, <span class="number">222</span>, <span class="number">1380</span>, <span class="number">231</span>, <span class="number">1996</span>, <span class="number">2218</span>, <span class="number">109</span>, <span class="number">1102</span>, <span class="number">1717</span>, <span class="number">1349</span>, <span class="number">1248</span>, <span class="number">1642</span>, <span class="number">595</span>, <span class="number">374</span>, <span class="number">512</span>, <span class="number">2194</span>, <span class="number">1583</span>, <span class="number">1481</span>, <span class="number">1446</span>, <span class="number">1991</span>, <span class="number">701</span>, <span class="number">455</span>, <span class="number">1405</span>, <span class="number">1478</span>, <span class="number">1662</span>, <span class="number">262</span>, <span class="number">1720</span>, <span class="number">583</span>, <span class="number">1900</span>, <span class="number">1203</span>, <span class="number">1209</span>, <span class="number">1786</span>, <span class="number">499</span>, <span class="number">1672</span>, <span class="number">513</span>, <span class="number">569</span>, <span class="number">2195</span>, <span class="number">396</span>, <span class="number">1758</span>, <span class="number">1516</span>, <span class="number">1055</span>, <span class="number">2000</span>, <span class="number">1570</span>, <span class="number">158</span>, <span class="number">2343</span>, <span class="number">1344</span>, <span class="number">293</span>, <span class="number">2305</span>, <span class="number">1327</span>, <span class="number">2155</span>, <span class="number">913</span>, <span class="number">572</span>, <span class="number">899</span>, <span class="number">1087</span>, <span class="number">2272</span>, <span class="number">393</span>, <span class="number">1726</span>, <span class="number">1712</span>, <span class="number">1276</span>, <span class="number">420</span>, <span class="number">1648</span>, <span class="number">812</span>, <span class="number">73</span>, <span class="number">2211</span>, <span class="number">956</span>, <span class="number">1196</span>, <span class="number">1441</span>, <span class="number">1403</span>, <span class="number">2297</span>, <span class="number">759</span>, <span class="number">116</span>, <span class="number">52</span>, <span class="number">887</span>, <span class="number">196</span>, <span class="number">1145</span>, <span class="number">1839</span>, <span class="number">1620</span>, <span class="number">1981</span>, <span class="number">2354</span>, <span class="number">2223</span>, <span class="number">1581</span>, <span class="number">1933</span>, <span class="number">2081</span>, <span class="number">1527</span>, <span class="number">1670</span>, <span class="number">1050</span>, <span class="number">1056</span>, <span class="number">2068</span>, <span class="number">36</span>, <span class="number">1108</span>, <span class="number">525</span>, <span class="number">246</span>, <span class="number">2349</span>, <span class="number">473</span>, <span class="number">1414</span>, <span class="number">1983</span>, <span class="number">1294</span>, <span class="number">1663</span>, <span class="number">2338</span>, <span class="number">618</span>, <span class="number">277</span>, <span class="number">1009</span>, <span class="number">1743</span>, <span class="number">873</span>, <span class="number">919</span>, <span class="number">1538</span>, <span class="number">1863</span>, <span class="number">1947</span>, <span class="number">798</span>, <span class="number">1273</span>, <span class="number">228</span>, <span class="number">951</span>, <span class="number">1386</span>, <span class="number">587</span>, <span class="number">2197</span>, <span class="number">2335</span>, <span class="number">128</span>, <span class="number">1362</span>, <span class="number">1358</span>, <span class="number">2149</span>, <span class="number">2319</span>, <span class="number">963</span>, <span class="number">364</span>, <span class="number">653</span>, <span class="number">1031</span>, <span class="number">1017</span>, <span class="number">1150</span>, <span class="number">681</span>, <span class="number">2283</span>, <span class="number">1105</span>, <span class="number">1258</span>, <span class="number">1306</span>, <span class="number">1506</span>, <span class="number">471</span>, <span class="number">194</span>, <span class="number">1658</span>, <span class="number">561</span>, <span class="number">518</span>, <span class="number">1799</span>, <span class="number">356</span>, <span class="number">1036</span>, <span class="number">2052</span>, <span class="number">601</span>, <span class="number">688</span>, <span class="number">1862</span>, <span class="number">389</span>, <span class="number">1149</span>, <span class="number">838</span>, <span class="number">264</span>, <span class="number">1916</span>, <span class="number">1022</span>, <span class="number">735</span>, <span class="number">1213</span>, <span class="number">435</span>, <span class="number">772</span>, <span class="number">2102</span>, <span class="number">950</span>, <span class="number">1472</span>, <span class="number">787</span>, <span class="number">714</span>, <span class="number">4</span>, <span class="number">699</span>, <span class="number">406</span>, <span class="number">1745</span>, <span class="number">1006</span>, <span class="number">2207</span>, <span class="number">832</span>, <span class="number">895</span>, <span class="number">1448</span>, <span class="number">846</span>, <span class="number">1830</span>, <span class="number">1366</span>, <span class="number">274</span>, <span class="number">2333</span>, <span class="number">1286</span>, <span class="number">1607</span>, <span class="number">1255</span>, <span class="number">708</span>, <span class="number">1378</span>, <span class="number">1181</span>, <span class="number">1902</span>, <span class="number">1113</span>, <span class="number">218</span>, <span class="number">174</span>, <span class="number">1693</span>, <span class="number">68</span>, <span class="number">1357</span>, <span class="number">1038</span>, <span class="number">804</span>, <span class="number">1311</span>, <span class="number">628</span>, <span class="number">138</span>, <span class="number">1594</span>, <span class="number">2225</span>, <span class="number">485</span>, <span class="number">139</span>, <span class="number">689</span>, <span class="number">1795</span>, <span class="number">1222</span>, <span class="number">847</span>, <span class="number">848</span>, <span class="number">912</span>, <span class="number">1186</span>, <span class="number">1451</span>, <span class="number">57</span>, <span class="number">1894</span>, <span class="number">1256</span>, <span class="number">1684</span>, <span class="number">2341</span>, <span class="number">304</span>, <span class="number">1749</span>, <span class="number">1576</span>, <span class="number">542</span>, <span class="number">469</span>, <span class="number">2115</span>, <span class="number">1597</span>, <span class="number">645</span>, <span class="number">1396</span>, <span class="number">1911</span>, <span class="number">2101</span>, <span class="number">1158</span>, <span class="number">410</span>, <span class="number">987</span>, <span class="number">453</span>, <span class="number">510</span>, <span class="number">606</span>, <span class="number">1845</span>, <span class="number">2091</span>, <span class="number">1647</span>, <span class="number">346</span>, <span class="number">2233</span>]</span><br><span class="line">train_index=a</span><br><span class="line">test_index=[x <span class="keyword">for</span> x <span class="keyword">in</span> list(range(len(X))) <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> train_index]</span><br><span class="line"><span class="comment">####################################################################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Data for training</span></span><br><span class="line">data_train=data4.iloc[train_index]</span><br><span class="line"><span class="comment">#Data for predicting</span></span><br><span class="line">data_test=data4.iloc[test_index]</span><br><span class="line"></span><br><span class="line"><span class="comment">#In the sample test</span></span><br><span class="line">Number_variables=[]</span><br><span class="line">OLS_R_2_IS=[] <span class="comment"># make a list to store our in the sample R_squared</span></span><br><span class="line">OLS_R_2_OS=[] <span class="comment">#make a list to store our out the sample R_squared</span></span><br><span class="line">depend_variable=[<span class="string">'mean_log'</span>,<span class="string">'gross_sq_feet_log'</span>]</span><br><span class="line">t=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Do a loop to run OLS after adding additional independent variables from 311 subsequently</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> ([list_comp[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(var)]):</span><br><span class="line"></span><br><span class="line">    t+=<span class="number">1</span></span><br><span class="line">    Number_variables.append(t)</span><br><span class="line"></span><br><span class="line">    depend_variable.append(j)</span><br><span class="line">    X=data_train[depend_variable]</span><br><span class="line">    y=data_train[<span class="string">'sale_price_log'</span>]</span><br><span class="line">    result=ols(y=y,x=X)</span><br><span class="line">    OLS_R_2_IS.append(result.r2)</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################################################</span></span><br><span class="line"><span class="comment">#Out of the sample test</span></span><br><span class="line"></span><br><span class="line">    temp=data_test[depend_variable]</span><br><span class="line"></span><br><span class="line">    a=np.array(temp)</span><br><span class="line">    b=np.array(result.beta)</span><br><span class="line">    c=np.sum(a*b[<span class="number">0</span>:<span class="number">-1</span>],axis=<span class="number">1</span>)+b[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    error=data_test[<span class="string">'sale_price_log'</span>]-c</span><br><span class="line">    R_2=<span class="number">1</span>-error.var()/data_test[<span class="string">'sale_price_log'</span>].var()</span><br><span class="line">    <span class="keyword">if</span> R_2&gt;<span class="number">0</span>:</span><br><span class="line">        OLS_R_2_OS.append(R_2)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        OLS_R_2_OS.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot</span></span><br><span class="line">pylab.title(<span class="string">'In sample R-squared vs out of sample R-squared'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_IS,<span class="string">'r'</span>,label=<span class="string">'R_squared(in the sample)'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_OS,<span class="string">'b'</span>,label=<span class="string">'R_squared(outof the sample)'</span>)</span><br><span class="line">pylab.legend(loc=<span class="string">'lower left'</span>)</span><br><span class="line">pylab.xlabel(<span class="string">'Number of independent variables'</span>)</span><br><span class="line">pylab.ylabel(<span class="string">'R-squared'</span>)</span><br><span class="line">pylab.draw()</span><br></pre></td></tr></table></figure>
<p><img src="NBsession2_files/NBsession2_69_0.png" alt="png"></p>
<p>So clearly, the new 311-related regressors add considerable value, however OLS runs into overfitting and start to crash after $t=90$ approximately. Let’s see if Ridge and Lasso can fix it</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Ridge=linear_model.Ridge(fit_intercept=<span class="keyword">True</span>,alpha=<span class="number">1</span>) <span class="comment">#try Ridge and Lasso with an arbitrary regularization parameter lambda</span></span><br><span class="line">Lasso=linear_model.Lasso(fit_intercept=<span class="keyword">True</span>,alpha=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X_train=np.matrix(data_train[depend_variable])</span><br><span class="line">y_train=np.array(data_train[<span class="string">'sale_price_log'</span>])</span><br><span class="line">X_test=np.matrix(data_test[depend_variable])</span><br><span class="line">y_test=np.array(data_test[<span class="string">'sale_price_log'</span>])</span><br><span class="line">Ridge.fit(X_train,y_train)</span><br><span class="line">Lasso.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># In the sample:</span></span><br><span class="line">p_IS=Ridge.predict(X_train)</span><br><span class="line">err_IS=p_IS-y_train</span><br><span class="line">R_2_IS_Ridge=<span class="number">1</span>-np.var(err_IS)/np.var(y_train)</span><br><span class="line"></span><br><span class="line">p_IS=Lasso.predict(X_train)</span><br><span class="line">err_IS=p_IS-y_train</span><br><span class="line">R_2_IS_Lasso=<span class="number">1</span>-np.var(err_IS)/np.var(y_train)</span><br><span class="line"></span><br><span class="line">Ridge_coef=Ridge.coef_</span><br><span class="line">Lasso_coef=Lasso.coef_</span><br><span class="line"><span class="comment">############################################################################    </span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#Out of sample</span></span><br><span class="line">p_OS=Ridge.predict(X_test)</span><br><span class="line">err_OS=p_OS-y_test</span><br><span class="line">R_2_OS_Ridge=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line"></span><br><span class="line">p_OS=Lasso.predict(X_test)</span><br><span class="line">err_OS=p_OS-y_test</span><br><span class="line">R_2_OS_Lasso=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line">print(<span class="string">"The R-squared we found for IS Ridge is: &#123;0&#125;"</span>.format(R_2_IS_Ridge))</span><br><span class="line">print(<span class="string">"The R-squared we found for OS Ridge is: &#123;0&#125;"</span>.format(R_2_OS_Ridge))</span><br><span class="line">print(<span class="string">"The R-squared we found for IS Lasso is: &#123;0&#125;"</span>.format(R_2_IS_Lasso))</span><br><span class="line">print(<span class="string">"The R-squared we found for OS Lasso is: &#123;0&#125;"</span>.format(R_2_OS_Lasso))</span><br></pre></td></tr></table></figure>
<pre><code>The R-squared we found for IS Ridge is: 0.519823192216
The R-squared we found for OS Ridge is: 0.488450370258
The R-squared we found for IS Lasso is: -2.22044604925e-16
The R-squared we found for OS Lasso is: 0.0
</code></pre><p>So Ridge works ok even with an arbitrary $\lambda=1$, while for Lasso tuning of the parameter is certainly needed. Let use the previous function to make it for both regularization models</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#select best lambda for Ridge</span></span><br><span class="line">lambdas = np.linspace(<span class="number">-15</span>,<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line">lambdas=[math.exp(i) <span class="keyword">for</span> i <span class="keyword">in</span> lambdas]</span><br><span class="line">lambda_r_optimal=Regularization_fit_lambda(<span class="number">1</span>,X_train,y_train,lambdas,p=<span class="number">0.4</span>,Graph=<span class="keyword">True</span>,logl=<span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">'Optimal lambda for Ridge=&#123;0&#125;'</span>.format(lambda_r_optimal))</span><br></pre></td></tr></table></figure>
<p><img src="NBsession2_files/NBsession2_73_0.png" alt="png"></p>
<pre><code>Optimal lambda for Ridge=0.00423383759375
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#select best lambda for Lasso</span></span><br><span class="line">lambdas = np.linspace(<span class="number">-15</span>,<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line">lambdas=[math.exp(i) <span class="keyword">for</span> i <span class="keyword">in</span> lambdas]</span><br><span class="line">lambda_l_optimal=Regularization_fit_lambda(<span class="number">2</span>,X_train,y_train,lambdas,p=<span class="number">0.4</span>,Graph=<span class="keyword">True</span>,logl=<span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">'Optimal lambda for Lasso=&#123;0&#125;'</span>.format(lambda_r_optimal))</span><br></pre></td></tr></table></figure>
<p><img src="NBsession2_files/NBsession2_74_0.png" alt="png"></p>
<pre><code>Optimal lambda for Lasso=0.00423383759375
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Ridge=linear_model.Ridge(fit_intercept=<span class="keyword">True</span>,alpha=lambda_r_optimal) <span class="comment">#redo Ridge and Lasso with selected regularization parameter lambda</span></span><br><span class="line">Lasso=linear_model.Lasso(fit_intercept=<span class="keyword">True</span>,alpha=lambda_l_optimal)</span><br><span class="line"></span><br><span class="line">X_train=np.matrix(data_train[depend_variable])</span><br><span class="line">y_train=np.array(data_train[<span class="string">'sale_price_log'</span>])</span><br><span class="line">X_test=np.matrix(data_test[depend_variable])</span><br><span class="line">y_test=np.array(data_test[<span class="string">'sale_price_log'</span>])</span><br><span class="line">Ridge.fit(X_train,y_train)</span><br><span class="line">Lasso.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># In the sample:</span></span><br><span class="line">p_IS=Ridge.predict(X_train)</span><br><span class="line">err_IS=p_IS-y_train</span><br><span class="line">R_2_IS_Ridge=<span class="number">1</span>-np.var(err_IS)/np.var(y_train)</span><br><span class="line"></span><br><span class="line">p_IS=Lasso.predict(X_train)</span><br><span class="line">err_IS=p_IS-y_train</span><br><span class="line">R_2_IS_Lasso=<span class="number">1</span>-np.var(err_IS)/np.var(y_train)</span><br><span class="line"></span><br><span class="line">Ridge_coef=Ridge.coef_</span><br><span class="line">Lasso_coef=Lasso.coef_</span><br><span class="line"><span class="comment">############################################################################    </span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#Out of sample</span></span><br><span class="line">p_OS=Ridge.predict(X_test)</span><br><span class="line">err_OS=p_OS-y_test</span><br><span class="line">R_2_OS_Ridge=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line"></span><br><span class="line">p_OS=Lasso.predict(X_test)</span><br><span class="line">err_OS=p_OS-y_test</span><br><span class="line">R_2_OS_Lasso=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line">print(<span class="string">"The R-squared we found for IS Ridge is: &#123;0&#125;"</span>.format(R_2_IS_Ridge))</span><br><span class="line">print(<span class="string">"The R-squared we found for OS Ridge is: &#123;0&#125;"</span>.format(R_2_OS_Ridge))</span><br><span class="line">print(<span class="string">"The R-squared we found for IS Lasso is: &#123;0&#125;"</span>.format(R_2_IS_Lasso))</span><br><span class="line">print(<span class="string">"The R-squared we found for OS Lasso is: &#123;0&#125;"</span>.format(R_2_OS_Lasso))</span><br></pre></td></tr></table></figure>
<pre><code>The R-squared we found for IS Ridge is: 0.618653826565
The R-squared we found for OS Ridge is: 0.568764752496
The R-squared we found for IS Lasso is: 0.597422426435
The R-squared we found for OS Lasso is: 0.563935422612
</code></pre><p>This is much better now - we can see that R2 values are much improved - both Lasso and Ridge provide 56% OS R2. And as IS and OS R2 are close there is almost no oferfitting left - both Lasso and Ridge manage it efficiently. Finally let’s reproduce the learning curves - IS/OS R2 depending on t for all three models - OLS, Lasso and Ridge - in order to track the dynamics. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#In the sample test</span></span><br><span class="line">Number_variables=[]</span><br><span class="line">OLS_R_2_IS=[] <span class="comment"># make a list to store our in the sample R_squared</span></span><br><span class="line">Ridge_R_2_IS=[]</span><br><span class="line">Lasso_R_2_IS=[]</span><br><span class="line">OLS_R_2_OS=[] <span class="comment">#make a list to store our out the sample R_squared</span></span><br><span class="line">Ridge_R_2_OS=[]</span><br><span class="line">Lasso_R_2_OS=[]</span><br><span class="line">depend_variable=[<span class="string">'mean_log'</span>,<span class="string">'gross_sq_feet_log'</span>]</span><br><span class="line">t=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Do a loop to run OLS after adding additional independent variables from 311 subsequently</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> ([list_comp[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(var)]):</span><br><span class="line"></span><br><span class="line">    t+=<span class="number">1</span></span><br><span class="line">    Number_variables.append(t)</span><br><span class="line"></span><br><span class="line">    depend_variable.append(j)</span><br><span class="line"><span class="comment"># In the sample:</span></span><br><span class="line"></span><br><span class="line">    X=data_train[depend_variable]</span><br><span class="line">    y=data_train[<span class="string">'sale_price_log'</span>]</span><br><span class="line">    result=ols(y=y,x=X)</span><br><span class="line">    OLS_R_2_IS.append(result.r2)</span><br><span class="line">    </span><br><span class="line">    X=np.matrix(data_train[depend_variable])</span><br><span class="line">    y=np.array(data_train[<span class="string">'sale_price_log'</span>])</span><br><span class="line">    Ridge=linear_model.Ridge(fit_intercept=<span class="keyword">True</span>,alpha=lambda_r_optimal)</span><br><span class="line">    Lasso=linear_model.Lasso(fit_intercept=<span class="keyword">True</span>,alpha=lambda_l_optimal)</span><br><span class="line">    Ridge.fit(X,y)</span><br><span class="line">    Lasso.fit(X,y)</span><br><span class="line">    p_IS=Ridge.predict(X)</span><br><span class="line">    err_IS=p_IS-y</span><br><span class="line">    R_2_IS_Ridge=<span class="number">1</span>-np.var(err_IS)/np.var(y)</span><br><span class="line">    p_IS=Lasso.predict(X)</span><br><span class="line">    err_IS=p_IS-y</span><br><span class="line">    R_2_IS_Lasso=<span class="number">1</span>-np.var(err_IS)/np.var(y)</span><br><span class="line">    Ridge_R_2_IS.append(R_2_IS_Ridge)</span><br><span class="line">    Lasso_R_2_IS.append(R_2_IS_Lasso)</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################################################</span></span><br><span class="line"><span class="comment">#Out of the sample test</span></span><br><span class="line"></span><br><span class="line">    temp=data_test[depend_variable]</span><br><span class="line"></span><br><span class="line">    a=np.array(temp)</span><br><span class="line">    b=np.array(result.beta)</span><br><span class="line">    c=np.sum(a*b[<span class="number">0</span>:<span class="number">-1</span>],axis=<span class="number">1</span>)+b[<span class="number">-1</span>]</span><br><span class="line">    </span><br><span class="line">    X_test=np.matrix(data_test[depend_variable])</span><br><span class="line">    y_test=np.array(data_test[<span class="string">'sale_price_log'</span>])</span><br><span class="line"></span><br><span class="line">    error=data_test[<span class="string">'sale_price_log'</span>]-c</span><br><span class="line">    R_2=<span class="number">1</span>-error.var()/data_test[<span class="string">'sale_price_log'</span>].var()</span><br><span class="line">    <span class="keyword">if</span> R_2&gt;<span class="number">0</span>:</span><br><span class="line">        OLS_R_2_OS.append(R_2)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        OLS_R_2_OS.append(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    p_OS=Ridge.predict(X_test)</span><br><span class="line">    err_OS=p_OS-y_test</span><br><span class="line">    R_2_OS_Ridge=<span class="number">1</span>-np.var(err_OS)/np.var(y_test)</span><br><span class="line"></span><br><span class="line">    p_OS=Lasso.predict(X_test)</span><br><span class="line">    err_OS=p_OS-y_test</span><br><span class="line">    R_2_OS_Lasso=<span class="number">1</span>-np.var(err_OS)/np.var(y_test) </span><br><span class="line">    Ridge_R_2_OS.append(R_2_OS_Ridge)</span><br><span class="line">    Lasso_R_2_OS.append(R_2_OS_Lasso)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot</span></span><br><span class="line">pylab.title(<span class="string">'In sample R-squared vs out of sample R-squared'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_IS,<span class="string">'g'</span>,label=<span class="string">'R_squared(in the sample)'</span>)</span><br><span class="line">pylab.plot(Number_variables,OLS_R_2_OS,<span class="string">'b'</span>,label=<span class="string">'R_squared(outof the sample)'</span>)</span><br><span class="line">pylab.plot(Number_variables,Ridge_R_2_IS,<span class="string">'k'</span>,label=<span class="string">'Ridge_R_squared(IS)'</span>)</span><br><span class="line">pylab.plot(Number_variables,Ridge_R_2_OS,<span class="string">'r'</span>,label=<span class="string">'Ridge_R_squared(OS)'</span>)</span><br><span class="line">pylab.plot(Number_variables,Lasso_R_2_IS,<span class="string">'y'</span>,label=<span class="string">'Lasso_R_squared(IS)'</span>)</span><br><span class="line">pylab.plot(Number_variables,Lasso_R_2_OS,<span class="string">'c'</span>,label=<span class="string">'Lasso_R_squared(OS)'</span>)</span><br><span class="line">pylab.legend(loc=<span class="string">'lower left'</span>)</span><br><span class="line">pylab.xlabel(<span class="string">'Number of independent variables'</span>)</span><br><span class="line">pylab.ylabel(<span class="string">'R-squared'</span>)</span><br><span class="line">pylab.draw()</span><br></pre></td></tr></table></figure>
<p><img src="NBsession2_files/NBsession2_77_0.png" alt="png"></p>
<p>So we see that Lasso and Ridge perform perfectly stable and unlike OLS, their OS R2 steadily grows with $t$</p>
<p>Finally look at the coefficients of Lasso performing feature selection for us - most of the coefficients are zero while the non-zero ones indicate the regressors left in the model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Lasso_coef</span><br></pre></td></tr></table></figure>
<pre><code>array([  0.34144248,   0.69046956,  -0.        ,   0.        ,
        -0.        ,  -0.        ,   0.        ,  -0.        ,
         0.        ,  -0.        ,   0.        ,   0.        ,
         0.        ,  -0.5262977 ,   0.        ,   0.        ,
         0.        ,   4.37823538,   0.        ,   0.        ,
        -0.        ,  -0.        ,  -0.        ,  -0.        ,
        -0.        ,  -0.        ,   0.        ,   0.        ,
        -0.        ,   7.28434482,   2.97783897,   0.        ,
         0.67294255,  -4.66543363,  -3.8401933 ,  -0.        ,
         0.        ,   0.        ,  -0.        ,  -0.        ,
         0.        ,  -2.0370214 ,  -0.        ,   7.55328286,
        -0.        ,   0.        ,  -0.        ,  -0.        ,
        -0.        ,  -0.        ,  -0.        ,  -0.        ,
         0.        ,   0.        ,   0.        ,   0.        ,
         0.55266874,   0.        ,   3.90614547,   0.        ,
         0.        ,   0.        ,  -0.31984493,   0.        ,
         0.        ,   0.        ,   0.        ,   0.        ,
         0.        ,   0.        ,  -0.        ,  -0.        ,
         2.76737771,  -0.        ,   0.        ,  -0.        ,
        -0.        ,   0.        ,   0.        ,   0.        ,
         0.        ,   0.        ,   0.        ,   0.        ,
        -1.22035937,  -0.        ,  -0.        ,   9.8401366 ,
         1.23997538,   0.        ,  -0.        ,   0.        ,
         5.61773108,  -2.24938118,   0.        ,   0.        ,
         0.        ,  -0.        ,  -0.        ,  -1.13927305,
        -0.        ,   0.        ,  -0.        ,   0.        ,
        -0.42705197,  -0.        ,   0.        ,  -0.        ,
        -0.        ,   0.        ,   0.        ,   0.        ,
        -0.        ,  -0.        ,   0.        ,  -0.        ,
        -1.09762973,  -0.        ,   0.        ,   0.        ,
         0.        ,  -0.        ,   0.        ,   0.        ,
         0.        ,  -0.        ,   0.        ,  -0.        ,
        -0.        ,   0.        ,  -0.        ,  -0.        ,
        -2.50885547,  -0.        ,  -1.40233476,  -2.69872158,
         0.55530533,   0.        ,   0.        ,  -0.        ,
        -0.        ,   0.        ,   0.        ,   4.75765015,
         0.        ,   2.62947147,  -0.        ,  -0.        ,
        -0.        ,  -0.        ,  -0.        ,  -0.        ,
         0.        , -15.23536609,  -0.        ,  -0.        ,
        -0.29216672,   0.        ,  -1.70026056,   0.        ,  -0.        ])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

  </section>

  
  
</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

</body>
</html>
